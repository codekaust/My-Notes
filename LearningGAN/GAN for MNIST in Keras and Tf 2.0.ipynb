{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following: https://towardsdatascience.com/demystifying-gans-in-tensorflow-2-0-9890834ab3d9\n",
    "\n",
    "Tf 2.0 for building network and adverserial processes.\n",
    "Numpy to generate noise.\n",
    "Matplotlib for saving images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow: 2.0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "\n",
    "print(\"Tensorflow: \" + tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few global variables required in the code that follows\n",
    "BATCH_SIZE = 256\n",
    "BUFFER_SIZE = 60000\n",
    "EPOCHES = 300\n",
    "OUTPUT_DIR = \"img\" # To store images by generator during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# Load mnist dataset\n",
    "mnist = keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "print(train_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before implementing neural network, the data are passed into a `tf.data.Dataset` object. It represents sequence of elements, in which each element contains one or more Tensor object — we’ll use it as an iterator to store our images in batches and loop trough them later. In addition, the images are normalized to be between -1 and 1 (same range which is generated by the uniform distribution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   3.  18.\n",
      "   18.  18. 126. 136. 175.  26. 166. 255. 247. 127.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.  30.  36.  94. 154. 170. 253.\n",
      "  253. 253. 253. 253. 225. 172. 253. 242. 195.  64.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.  49. 238. 253. 253. 253. 253. 253.\n",
      "  253. 253. 253. 251.  93.  82.  82.  56.  39.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.  18. 219. 253. 253. 253. 253. 253.\n",
      "  198. 182. 247. 241.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.  80. 156. 107. 253. 253. 205.\n",
      "   11.   0.  43. 154.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.  14.   1. 154. 253.  90.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 139. 253. 190.\n",
      "    2.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  11. 190. 253.\n",
      "   70.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  35. 241.\n",
      "  225. 160. 108.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  81.\n",
      "  240. 253. 253. 119.  25.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   45. 186. 253. 253. 150.  27.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.  16.  93. 252. 253. 187.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0. 249. 253. 249.  64.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   46. 130. 183. 253. 253. 207.   2.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  39. 148.\n",
      "  229. 253. 253. 253. 250. 182.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  24. 114. 221. 253.\n",
      "  253. 253. 253. 201.  78.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.  23.  66. 213. 253. 253. 253.\n",
      "  253. 198.  81.   2.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.  18. 171. 219. 253. 253. 253. 253. 195.\n",
      "   80.   9.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.  55. 172. 226. 253. 253. 253. 253. 244. 133.  11.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0. 136. 253. 253. 253. 212. 135. 132.  16.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]]\n",
      "[[-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -0.9764706  -0.85882354 -0.85882354 -0.85882354 -0.01176471  0.06666667\n",
      "   0.37254903 -0.79607844  0.3019608   1.          0.9372549  -0.00392157\n",
      "  -1.         -1.         -1.         -1.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -0.7647059  -0.7176471  -0.2627451   0.20784314\n",
      "   0.33333334  0.9843137   0.9843137   0.9843137   0.9843137   0.9843137\n",
      "   0.7647059   0.34901962  0.9843137   0.8980392   0.5294118  -0.49803922\n",
      "  -1.         -1.         -1.         -1.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -0.6156863   0.8666667   0.9843137   0.9843137   0.9843137\n",
      "   0.9843137   0.9843137   0.9843137   0.9843137   0.9843137   0.96862745\n",
      "  -0.27058825 -0.35686275 -0.35686275 -0.56078434 -0.69411767 -1.\n",
      "  -1.         -1.         -1.         -1.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -0.85882354  0.7176471   0.9843137   0.9843137   0.9843137\n",
      "   0.9843137   0.9843137   0.5529412   0.42745098  0.9372549   0.8901961\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -0.37254903  0.22352941 -0.16078432  0.9843137\n",
      "   0.9843137   0.60784316 -0.9137255  -1.         -0.6627451   0.20784314\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -0.8901961  -0.99215686  0.20784314\n",
      "   0.9843137  -0.29411766 -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.          0.09019608\n",
      "   0.9843137   0.49019608 -0.9843137  -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -0.9137255\n",
      "   0.49019608  0.9843137  -0.4509804  -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -0.7254902   0.8901961   0.7647059   0.25490198 -0.15294118 -0.99215686\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -0.3647059   0.88235295  0.9843137   0.9843137  -0.06666667\n",
      "  -0.8039216  -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -0.64705884  0.45882353  0.9843137   0.9843137\n",
      "   0.1764706  -0.7882353  -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -0.8745098  -0.27058825  0.9764706\n",
      "   0.9843137   0.46666667 -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.          0.9529412\n",
      "   0.9843137   0.9529412  -0.49803922 -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -0.6392157   0.01960784  0.43529412  0.9843137\n",
      "   0.9843137   0.62352943 -0.9843137  -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -0.69411767  0.16078432  0.79607844  0.9843137   0.9843137   0.9843137\n",
      "   0.9607843   0.42745098 -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -0.8117647  -0.10588235\n",
      "   0.73333335  0.9843137   0.9843137   0.9843137   0.9843137   0.5764706\n",
      "  -0.3882353  -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -0.81960785 -0.48235294  0.67058825  0.9843137\n",
      "   0.9843137   0.9843137   0.9843137   0.5529412  -0.3647059  -0.9843137\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -0.85882354  0.34117648  0.7176471   0.9843137   0.9843137   0.9843137\n",
      "   0.9843137   0.5294118  -0.37254903 -0.92941177 -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.        ]\n",
      " [-1.         -1.         -1.         -1.         -0.5686275   0.34901962\n",
      "   0.77254903  0.9843137   0.9843137   0.9843137   0.9843137   0.9137255\n",
      "   0.04313726 -0.9137255  -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.        ]\n",
      " [-1.         -1.         -1.         -1.          0.06666667  0.9843137\n",
      "   0.9843137   0.9843137   0.6627451   0.05882353  0.03529412 -0.8745098\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.         -1.         -1.\n",
      "  -1.         -1.         -1.         -1.        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: (None, 784), types: tf.float32>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images = train_images.astype(\"float32\")\n",
    "print(train_images[0])\n",
    "\n",
    "train_images = (train_images - 127.5) / 127.5\n",
    "\n",
    "print(train_images[0])\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images.reshape(train_images.shape[0],784)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will use tf.keras api for building generator and discriminator classes. In the constructor we are going to define the layers of the network and in the call method the forward pass of the model. We are going to input 100-dimensional noise into the network and output a vector of the size 784. Later, we are going reshape the vector back to a matrix with the dimension of 28x28 (the original size of the images). In addition, the generate_noise method is used to create random data points from the uniform distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(keras.Model):\n",
    "    \n",
    "    def __init__(self, random_noise_size = 100):\n",
    "        super().__init__(name='generator')\n",
    "        #layers\n",
    "        self.input_layer = keras.layers.Dense(units = random_noise_size)\n",
    "        self.dense_1 = keras.layers.Dense(units = 128)\n",
    "        self.leaky_1 =  keras.layers.LeakyReLU(alpha = 0.01)\n",
    "        self.dense_2 = keras.layers.Dense(units = 128)\n",
    "        self.leaky_2 = keras.layers.LeakyReLU(alpha = 0.01)\n",
    "        self.dense_3 = keras.layers.Dense(units = 256)\n",
    "        self.leaky_3 = keras.layers.LeakyReLU(alpha = 0.01)\n",
    "        self.output_layer = keras.layers.Dense(units=784, activation = \"tanh\")\n",
    "        \n",
    "    def call(self, input_tensor):\n",
    "        ## Definition of Forward Pass\n",
    "        x = self.input_layer(input_tensor)\n",
    "        x = self.dense_1(x)\n",
    "        x = self.leaky_1(x)\n",
    "        x = self.dense_2(x)\n",
    "        x = self.leaky_2(x)\n",
    "        x = self.dense_3(x)\n",
    "        x = self.leaky_3(x)\n",
    "        return  self.output_layer(x)\n",
    "    \n",
    "    def generate_noise(self,batch_size, random_noise_size):\n",
    "        return np.random.uniform(-1,1, size = (batch_size, random_noise_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Noise using generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective of Discriminator\n",
    "![](https://miro.medium.com/max/910/1*hkgSodUw0ynuhNz8xk-Npg.png)\n",
    "\n",
    "### Objective of Generator\n",
    "![](https://miro.medium.com/max/536/1*afFPOyuuldrB4maAljuNHw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective function described above is nothing else then binary cross entropy. It takes the noise which is feed into the discriminator and only true labels, because the generator thinks that he produces real images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits = True)\n",
    "\n",
    "def generator_objective(dx_of_gx):\n",
    "    # Labels are true here because generator thinks he produces real images. \n",
    "    return cross_entropy(tf.ones_like(dx_of_gx), dx_of_gx) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing the same for the discriminator objective but now we adding the fake and the real loss together. In addition, we add a little bit smoothing to the objective of the real loss, to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_objective(d_x, g_z, smoothing_factor = 0.9):\n",
    "    \"\"\"\n",
    "    d_x = real output\n",
    "    g_z = fake output\n",
    "    \"\"\"\n",
    "    real_loss = cross_entropy(tf.ones_like(d_x) * smoothing_factor, d_x) # If we feed the discriminator with real images, we assume they all are the right pictures --> Because of that label == 1\n",
    "    fake_loss = cross_entropy(tf.zeros_like(g_z), g_z) # Each noise we feed in are fakes image --> Because of that labels are 0. \n",
    "    total_loss = real_loss + fake_loss\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator network\n",
    "# Input = 784 dim vector, Output = one neuron (tells if image fake or real)\n",
    "\n",
    "class Discriminator(keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__(name = \"discriminator\")\n",
    "        \n",
    "        #Layers\n",
    "        self.input_layer = keras.layers.Dense(units = 784)\n",
    "        self.dense_1 = keras.layers.Dense(units = 128)\n",
    "        self.leaky_1 =  keras.layers.LeakyReLU(alpha = 0.01)\n",
    "        self.dense_2 = keras.layers.Dense(units = 128)\n",
    "        self.leaky_2 = keras.layers.LeakyReLU(alpha = 0.01)\n",
    "        self.dense_3 = keras.layers.Dense(units = 128)\n",
    "        self.leaky_3 = keras.layers.LeakyReLU(alpha = 0.01)\n",
    "        self.logits = keras.layers.Dense(units = 1)  # This neuron tells us if the input is fake or real\n",
    "    def call(self, input_tensor):\n",
    "          ## Definition of Forward Pass\n",
    "        x = self.input_layer(input_tensor)\n",
    "        x = self.dense_1(x)\n",
    "        x = self.leaky_1(x)\n",
    "        x = self.leaky_2(x)\n",
    "        x = self.leaky_3(x)\n",
    "        x = self.leaky_3(x)\n",
    "        x = self.logits(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = keras.optimizers.RMSprop()\n",
    "discriminator_optimizer = keras.optimizers.RMSprop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Step\n",
    "\n",
    "By using the gradients of the generator and discriminator we are training both networks simultaneously . First, some noise is generated according to the batch size we defined before. Afterwards we’re feeding the real image as well as the fake image into the discriminator and calculate its lost. The last step is to do the same for the generator and to apply those gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def training_step(generator: Generator, discriminator: Discriminator, images:np.ndarray , k:int =1, batch_size = 32):\n",
    "    for _ in range(k):\n",
    "         with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "            noise = generator.generate_noise(batch_size, 100)\n",
    "            g_z = generator(noise)\n",
    "            d_x_true = discriminator(images) # Trainable?\n",
    "            d_x_fake = discriminator(g_z) # dx_of_gx\n",
    "\n",
    "            discriminator_loss = discriminator_objective(d_x_true, d_x_fake)\n",
    "            # Adjusting Gradient of Discriminator\n",
    "            gradients_of_discriminator = disc_tape.gradient(discriminator_loss, discriminator.trainable_variables)\n",
    "            discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables)) # Takes a list of gradient and variables pairs\n",
    "\n",
    "\n",
    "            generator_loss = generator_objective(d_x_fake)\n",
    "            # Adjusting Gradient of Generator\n",
    "            gradients_of_generator = gen_tape.gradient(generator_loss, generator.trainable_variables)\n",
    "            generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = np.random.uniform(-1,1, size = (1, 100)) # generating some noise for the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(dataset, epoches):\n",
    "    for epoch in range(epoches*4):\n",
    "        for batch in dataset:\n",
    "            training_step(generator, discriminator, batch ,batch_size = BATCH_SIZE, k = 1)\n",
    "        ## After ith epoch plot image \n",
    "        if (epoch % 50) == 0: \n",
    "            fake_image = tf.reshape(generator(seed), shape = (28,28))\n",
    "            print(\"{}/{} epoches\".format(epoch, epoches))\n",
    "            #plt.imshow(fake_image, cmap = \"gray\")\n",
    "            plt.imsave(\"{}/{}.png\".format(OUTPUT_DIR,epoch),fake_image, cmap = \"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/300 epoches\n",
      "50/300 epoches\n",
      "100/300 epoches\n",
      "150/300 epoches\n",
      "200/300 epoches\n",
      "250/300 epoches\n",
      "300/300 epoches\n",
      "350/300 epoches\n",
      "400/300 epoches\n",
      "450/300 epoches\n",
      "500/300 epoches\n",
      "550/300 epoches\n",
      "600/300 epoches\n",
      "650/300 epoches\n",
      "700/300 epoches\n",
      "750/300 epoches\n",
      "800/300 epoches\n",
      "850/300 epoches\n",
      "900/300 epoches\n",
      "950/300 epoches\n",
      "1000/300 epoches\n",
      "1050/300 epoches\n",
      "1100/300 epoches\n",
      "1150/300 epoches\n"
     ]
    }
   ],
   "source": [
    "training(train_dataset, EPOCHES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f56ce28b780>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fake_image = generator(np.random.uniform(-1,1, size = (1, 100)))\n",
    "plt.imshow(tf.reshape(fake_image, shape = (28,28)), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
