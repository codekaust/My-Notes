Course Overview
Course Overview
[Autogenerated] Hi, everyone. My name is Paul O'Fallon. And welcome to my course. Cassandra four developers. Cassandra is an open source. No sequel database that has been used by some of the world's largest companies to deliver service is that span the globe. In this course, we'll start with the basics of how Cassandra works, including consistency levels and replication strategies. We'll spend most of the course going oversea que el or the Cassandra Query language. Before we're done, we'll cover some of the more recent additions to this database platform, such as materialized views and user defined functions. By the end of this course, you should have a good introduction to the options available to you as a developer leveraging Cassandra in your project. I hope you'll join me in this journey toe. Lauren Cassandra with the Cassandra for Developers Course that plural site.

What Is Cassandra?
History of Cassandra, Introducing Nodes, and Virtual Nodes
[Autogenerated] Hello. My name is Paul O'Fallon, and I'd like to welcome you to the course. Cassandra. Four developers. This first module will help answer the question. What is Cassandra? We'll start with a brief history of Cassandra and look at some of the very familiar names who are using the open source. No sequel database. Then we'll dive right in and examine the topology of a Cassandra cluster. Let's get started. Cassandra was originally developed at Facebook back in 2008. It was designed as the storage tear for a Facebook feature called Inbox Search. Such a feature required a storage system that could handle a tremendous number of rights as well as geographic replication. To reduce search, Leighton sees for end users. Like most modern distributed no sequel systems. Cassandra was founded on the principles outlined in two seminal papers on the topic. Google's big table in Amazon's Dynamo Cassandra combines the distributed nature of Dynamo and the data model of Big Table. In fact, if you look closely, one of the authors of the Dynamo paper is also one of the authors of the Facebook Cassandra paper Facebook Open sourced Cassandra in 2008 and in 2009 it became an Apache incubator project. In 2010 it graduated to a top level Apache project. You can download it from cassandra dot Apache dot org's. Many companies looking to operate at Internet scale have looked to Cassandra to meet their distributed storage needs. Netflix famously migrated from Oracle to Cassandra, running an Amazon's public cloud. Apple was also revealed to be a significant user of Cassandra, running more than 75,000 nodes in storing tens of petabytes of data. This diagram is probably the most common representation of a Cassandra installation. Each dot On the circle is a note, which represents a running instance of Cassandra. This diagram helps drive home the point that a Cassandra cluster is a true master lis peer to peer system. With no single point of failure. All notes can perform. All of Cassandra's functions will spend most of this course talking about how to structure our data in Cassandra. But first we'll cover a few topics related to ____. Cassandra operates under the hood. First, we'll look at how data is stored in a Cassandra cluster. All data stored in Cassandra is associated with a token. There are astronomical number of possible token values. As you can see from the range shown here when there is only a single Cassandra node, this one note is responsible for storing the data associated with all possible token values. Then, when a second notice added, this new note takes on responsibility for the data associated with a contiguous set of token values. As new nodes are added, this process continues, each node taking over a contiguous range of token values and storing the data associated with it for 1/4 node and 1/5 node. And the sixth note. One of the challenges of this approach is what happens when a new note is added to the cluster. There are only so many existing notes it can consult in order to populate itself. In our example here, our sixth note is taking over a range of tokens previously owned by Nodes one and four. Assuming for simplicity's sake that only one copy of the data is being stored, there are only two places for notes. Six to go to populate itself, notes one and four with Cassandra one dot to a new concept was introduced virtual notes or V notes for short with V notes. Each node defines a number of virtual notes thes V notes, then each take on a much smaller contiguous range of tokens. In the case of a single note, it's still largely the same. However, even as this one note is still storing all the data, it allocates the entire token range to its own V notes. Now, when a new note is added, this second note still takes ownership of a percentage of the tokens. However, it's no longer a contiguous range of token values. Each V node takes over a contiguous range of tokens at a random spot in the total range of token values. In our example here, each note is adding four virtual notes to the token space. When we've added our sixth node. We now have a total of 24 V notes covering the range of possible tokens, while in the end it's still six instances of Cassandra responsible for the physical storage of the data. The layer of indirection provided by V notes has a couple of benefits. Comparing this to our earlier example. When we add note six to the cluster, it becomes responsible for four smaller, contiguous token ranges those assigned to each of its V nodes. Now it's taken over tokens previously owned by four different notes not to. In order to populate itself, it now has twice a CZ many notes to talk to. And this is just for a six note cluster. The V note approach on lee gets better as you add more notes to the cluster. So now that we have our six note cluster with 24 V nodes, how do we know where it will store a piece of data? This is where the partition ER comes in. Cassandra is a partitioned rose store, which means that data is red or written with the partition key. When data is inserted, the partition KIIS hashed into a token. The token value determines which V node an actual physical node will be responsible for the data. In this case, it's no one. Okay, enough slides. Let's take a look at some of this in action

Demo: A Basic Cassandra Cluster in Docker
[Autogenerated] in this demo, we're going to stand up a Cassandra cluster. Well, then take a look at the no tool cli that comes with Cassandra. And while we're here, we'll also check out the cassandra dot Yamil configuration file. Throughout, this course will be running both single Cassandra notes as well as entire clusters to keep it simple. We're going to do this with Docker to follow along. You'll want to install Doctor Desktop for your platform. The free community edition is fine. Installing this will give us the Docker and Dr Composed command line tools will be using throughout the course. Some of the early demos in this course will require a fair amount of memory more than doctor is allocated by default. After installing and running Docker desktop, find the icon in your tool bar or trey and choose preferences were settings and then advanced. Increase the memory to four gigabytes and raise the swap to one and 1/2 gigabytes. In my experience, these are the minimum settings required to run all the demos in this course. If you don't have enough memory to do this, don't worry. You can just watch the first few demos where we spend open entire cluster later, demos will only use a single Cassandra note, which doesn't require that much memory are three. Note. Cassandra Cluster is defined in a docker composed the ammo file that's included with the course here. Each service will be a different node in our cluster. Noticed that for notes, two and three were setting an environment variable Cassandra seeds. When we launched these containers, they will need a way to bootstrap themselves in order to join the cluster. If you're adding a new note to an existing cluster, you need to tell it the seed nodes it can talk to in order to learn about the cluster. Don't let this diminish your opinion of Cassandra as a master list. Peer to peer system it ISS Any number of notes in your cluster can act as seed notes. These function as nodes in every other way, but they also help bootstrap. New notes coming into the cluster will launch our first Cassandra node with Docker composed up Dash D In one. I've already downloaded the necessary doctor image, so you may see many more messages. Scroll by as you download the image for the first time Docker PS shows the container up and running. Next, we're going to execute a Cassandra Command on the container itself. Cassandra comes with a command line tool called Node Tool, which allows you to find out a lot about your cluster and to administer it as well. Here, we're going to run no tool status to see if this Cassandra note is up and running yet. Okay, let's see what we have here on the far left. You see you in the U means this notice up and the n means that it's in a normal state, which is just what we would hope for. Next is the I. P address of this node. As a signed by Dr Look at the Tokens column. See the 2 56 Even though this says tokens, it's really the number of the notes presented by this node. In our slides, we used four per note, but the default is much higher. As an aside, you'll find the word token used to mean to slightly different things when talking about Cassandra. In some cases, it's an indication of a range of token values, as it is here, and other times it's referring to an individual token, we'll save some of the other information presented here for a later demo. Okay, now we're going to run another no tool command, no tool ring. This command displays a list of token ranges allocated across the nodes. Based on the terminology we've used so far, each row here equates to a virtual node. The columns we want to focus on Here are the first and last columns. The first column is the I. P. Address of the node that this row or V note belongs to. You'll notice that since we only have one node, these all have the same I p address. The last column is the token value, representing the end of the token range assigned to this V node. Next, we'll look at the cassandra dot yellow configuration file in our container. This file has a tremendous number of knobs and switches you can use to configure your Cassandra Cluster. The one we're going to look at is the num tokens property. This is where the number of virtual nodes were. Token ranges are set for this node. Noticed that the default is the 2 56 we saw when we ran the node tool Status Command. Take a moment to browse through the file. It's very well documented and is a learning experience on its own. Next will add another note to our cluster by running Docker composed up Dash D in two. Great Now Dr P s shows two containers running If we run no tool status, we see two rows. But our new note has a status of J, meaning that it's still in the process of joining the cluster and bringing another 256 virtual notes to the party running no tools status again. That's still joining one more time, and we see that it's finished joining, and we now have a two node cluster. One side note. The no tool Status command can be run on any note in the cluster. Here is the result of running it on note into the same as on in one. Okay, so let's run no tool ring with two nodes in the cluster. Now things get a little more interesting. Note that there are two different I P addresses in the first column meeting that our token ranges are now randomly assigned to 512 e notes across two nodes. Let's log in to note into and look at its cassandra dot Thiemo file. This time we're going to look for the seeds property. This doctor image uses the Cassandra Seeds environment variable to set the seeds property here just for good measure. Let's run one more doctor container and add 1/3 node to our cluster. As we would expect, Dr. P s shows all three containers running our first indication of no tool status. And the new note hasn't even shown up yet. Running in again shows our third node joining the cluster one more time, and we see that it's finished. Now we're up to three nodes, one more run of no tool ring, and we see all three I P addresses represented in the first column. The entire range of token values is now allocated to 768 virtual notes across three physical notes like I mentioned before, no tool packs quite a punch. Take a minute to run no tools, help and peruse the options. Most of these are operational in nature and outside the scope of this course. However, if nothing else, it will give you an idea of how much there is to learn and explore about Cassandra before we wrap up our demo. Let's shut down our cluster with Docker composed down. Great. Now we'll have a clean slate when we start our next demo.

Snitches
[Autogenerated] the next topic will cover is a snitch. The snitch is what Cassandra uses to gain an understanding of the environment, physical or virtual, in which the cluster is being run. It's used to efficiently route requests and is consulted when storing multiple copies of the data. The default and aptly named Simple Snitch is suitable for development in single data center environments. A much more interesting snitch is the gossiping property file. Snitch. Gossip is the protocol, Cassandra notes, used to talk to one another and keep everyone up to date on the state of the cluster. This snitch uses a property file deployed with each node to describe that nodes location. Then the notes gossip this information to each other. A snitch like this can tell you when nodes are indifferent racks. It can also distinguish entire data centers from one another. This two tiered hierarchy data centers and racks can represent their true physical equivalents, or I can simply be used as a logical grouping of nodes. Other snitches include the Property File snitch, where an entire topology is defined in a single file Cloud and other infrastructure providers have snitches of their own mapping data centers and racks to their own proprietary constructs. For example, the easy to multi region snitch equates data centers to AWS regions and racks to availability zones. Okay, let's revisit our last demo and this time spin up a multi data center cluster.

Demo: A Multi-datacenter Cluster in Docker
[Autogenerated] in this demo, we're going to stand up a multi data center, Cassandra Cluster. Along the way. We'll keep using are no tool cli and explore a couple of Cassandra's configuration files. Let's get started looking at our doctor composed file. We see that things here are a little different. We're setting three new environment variables on each container. We're setting the endpoint snitch to the gossiping property file snitch and then setting the data center and rack for each node in the cluster. Let's spend up our first new like before Checking Dr P s again to see that the container is running. Next will run no tool status to check on our first node. Great. Our first note is up and running. Noticed This time, though, that the data center value at the top is D. C. One and the rack on the bottom right is our A C one. These are the values we set as environment variables in our doctor composed file. Now we'll start our second instance of Cassandra Great. Our second container has been launched. If we run no tool status, we'll see that our second notice still joining the cluster looking at the bottom right, though we see that this new note is in a different rack R A C two running no tool status. A couple of more times, we see that our second note has finished joining the cluster. Now that we have two nodes, let's run no tool ring and check out our token allocations. As we expect, we see the tokens allocated to RV notes spread across to physical notes. What's different this time is, since we have notes in two different racks, we see both our A c one and R A C to show up here each alongside their corresponding node I p address. Next, let's look at the Cassandra configuration in our doctor container. First looking at Cassandra Dottie Amul, we see the endpoint snitch set to use gossiping property file Snitch. You'll notice. Along the way, we passed a good description of some of the other snitches available. Next, let's check out the file that holds our data center and rack information for this node. This is found in the Cassandra Rack D. C. Properties file. Here is where our D. C and RACK Environment variables are captured and made available to Cassandra. Each node reads this file on startup and gossips this information out to the rest of the cluster. Now let's add one more note to our cluster. This time we're adding the node in a new data center with the container started. Let's run no tool status Notice. This new note is listed under its own data center, DC to and it's still in joining status. One more check of the status and it's now up. We're officially running a multi data center. Cassandra Cluster. Next let's run no tool ring on our new cluster and see what it looks like at the end of the command. It looks like all the token ranges are assigned to the same node. Well, in this case, they are. This is the one node in data center D. C. Too scrolling back through the results, we can see that the token range allocations are broken down by data center scrolling even further back, we can see the token ranges allocated to the two nodes in our first data center. This is important as it means, each logical data center has its own complete range of tokens which are allocated to the V notes provided by the nodes assigned to that data center. Finally, we'll clean up after ourselves by running Docker composed down

Conclusion
[Autogenerated] to wrap things up. Let's recap. We started this module with a brief history of Cassandra. Then we looked at how the data is spread around a Cassandra cluster, including the introduction of virtual nodes and partition er's. We saw how snitches can be used to describe the physical or logical layout of a series of Cassandra nodes. Along the way, we launched a couple of different Cassandra clusters, using Dr Both a single and a multi data center cluster. Finally, we used Cassandra's Note Tool Command to inspect our cluster. I hope this module has been a good introduction to Cassandra. Stick around for our next module, where we tackle replication and consistency.

Replication and Consistency
Replication Strategies
[Autogenerated] Hello. My name is Paul O'Fallon, and I'd like to welcome you to the next module in the course. Cassandra for developers in our last module, we looked at how a single copy of data is distributed across the cluster. In this module will look at using replication strategies to store multiple copies of data in a cluster. We'll also look at Cassandra's support for something called tunable consistency and how that gives us query level flexibility when reading and writing data into our cluster. Let's get started. We ended the discussion of virtual notes in Module one showing data being written to a specific V node owned by knowed one in our cluster in a Cassandra environment. It is common expected, really, that you will store multiple copies of the data on different notes throughout your cluster. This gives you increased reliability as well as performance. Not only can you more easily tolerate a node becoming unavailable, but in certain circumstances you may choose to read a specific copy of the data from a note, for example, in a data center geographically closer to the system making the query before we get too far into this module, let's stop for a minute and go over some basic Cassandra terminology at the highest level. Data in Cassandra is organized into key spaces. The closest analogy to this in the relation a ll world would be an oracle or my sequel table space within a Cassandra Key Space. There are one or more tables. A table here is a pretty close match, conceptually, at least to its relation A ll counterpart. From here we have partitions like we mentioned in module one. All data written to Cassandra is associated with a partition key. This partition key determines where the data is located in the cluster and all data and a partition is stored together. The partition is the primary interaction point when reading or writing data to Cassandra. Finally, data within a partition may be represented as one or more rose. The specifics of a replication strategy are determined at the key space level. So if the partition key is used to determine the location of the first copy of the data written to a Cassandra cluster, the key space settings are used to determine the number of copies of the data and where they are stored throughout the cluster. We're going to cover two strategies for configuring this replication. The first is simple strategy. True to its name, this is best used in development environments or single data center clusters. Here's an example of Sea Que El or a Cassandra Query language statement to create a key space with simple strategy replication. We're going to cover CQ Elmore in our next module, but this is pretty straightforward. Were asking Cassandra to store three copies of all the partitions in all the tables written to the cluster. In this key space. Simple enough, you'll see a pattern here with snitches and replication. Strategy's working hand in hand in a cluster configured with a simple snitch. This replication strategy is really all that's available. We can ask Cassandra to store multiple copies of our data, and it will do its best to store them on different nodes. But that's really it. More interesting is the network topology strategy, which you can see being configured here. Notice that now we're not simply specifying how many copies of the data to store, but instead were in numerator each data center and specifying how many copies of the data we want stored in each so in this case were storing four copies of the data for each partition in each table in the key space. But more than that, we're telling Cassandra we want three copies stored in Data Center one and only one copy in Data center, too. This replication strategy is ideal for production environments, especially multi data center clusters. It requires an understanding of the clusters topology, which requires the cluster to be configured with inappropriate snitch such as thedot sipping property file snitch or one specific to a cloud provider. Note that it not only puts the replicas of the data in multiple data centers as requested, but also uses the knowledge of racks to smartly distribute the replicas within each data center as well. Let's take a closer look at how to configure these two replications strategies and how they affect the distribution of data in a Cassandra Cluster

Demo: Replication Strategies
[Autogenerated] in this demo, we're going to look at key spaces and their replication strategies. We'll do this for both single and multi data center clusters. Let's get started. Our doctor composed is a little different than the one from our last module. So let's take a quick look. Notice that for service in one. We're building our own container and calling it Cassandra with CQ l s h r C. Then we're using this image for service is in two and and three as well. Everything else is the same. Next, let's look at the doctor file Used to build this image Pretty simple, really. We're starting from the same Cassandra image and just copying one additional file. Looking at that file were setting some time outs for the CQ l s H command line tool. In my testing of the demos for this course, I noticed that the default time outs were sometimes too short. So this will make them longer one minute each. Okay, Now, in our single d C folder, we're going to run Docker compose up Dash D, which will bring up the entire three note cluster. If your computer is anything like mine, this will take some time. You may have to run no tool status several times before all three are up and normal. Next, we're going to run CQ l s H on the first note in our cluster CQ l That sage is a command that comes with Cassandra and can be used to execute si que el statements against a Cassandra Cluster. This is analogous to say sequel plus for Oracle or the Mice Equal command line tool. We're going to be using CQ l s H for most of the remainder of this course. First, let's execute the same command we first saw in our slides will create the keys based plural site and set the replication to use simple strategy with a replication factor of three. Okay, that was simple enough. Now that we have our key space created, we're going to revisit our friend No tool. However, this time we're going to give it a new command, describe ring and provide it with the name of the key space we just created Plural site. Okay, so this is output we haven't seen from no tool before, so let's take a look. Each row of output represents a range of tokens. This is similar to the last module, when we ran no to a ring. But the output here is more explicit. Each row to finds a start and end token range because describe ring requires us to provide a key space, it's able to tell us where it will store each copy of the data for a particular token range. Note. Here it defines the endpoint details as including nodes 1 72.20 dot 0.23 and four. This is interesting, but not terribly so since we only have a three note cluster and we configured the plural psyche space to store three copies of the data, each token range will store its data on all three notes. We can show this a different way by running our original no tool status command but this time passing in the name of our key space as an additional parameter. Now let's look at a column in the results. We haven't been paying much attention to until now. The owns effective column here shows that each note owns 100% of the data for the plural site key space. This is because storing three copies of each piece of data on a three note cluster means each note will have a complete set of data for the key space. Now, to change things up a bit, we'll go back and re create our key space with a different configuration. Will run CQ l s H and drop the plural psyche space. Then we'll re create it with a similar create key space command. But this time we'll set the replication factor to one. You might not do this in production, but this will help us learn how Cassandra reacts to these replication settings. Now, if we run no tool status plural site again, we see that the owns effective column has changed. Now, each note on Lee owns approximately 1/3 of the data in the cluster. This is because we have a three note cluster and only one copy of the data is being stored for the plural psyche space. Therefore, each piece of data will only be written toe one node. Let's run our no tool describe bring plural site command again and see what that looks like. Noticed now for our endpoint details, we only have one entry for each token range This is the only location in the cluster where data falling within this token range will be written just a sanity check ourselves. Let's grab one of these end token values and go back to our old note to a ring command that we used in module one. If we run no tool ring without specifying a key space and look for that end token value, we see that the node I p address here matches the i p address from the previous describe. Bring command. Okay, we're done with this cluster. So let's clean up after ourselves by tearing it down. Next, we're going to look at the network topology strategy for replication. Taking a quick glance at the Docker composed Jamel in the multi D C directory, we're building the same doctor image here as well to set our CQ l s H timeout values. You'll notice, too, that this is a four note cluster with three nodes in data center D. C. One spread across two racks. Data center D C. Two, on the other hand, has only one note. We'll start this four note multi data center cluster with Dr Composed Up Dash D. This will probably take several minutes to start running. No tool status for the first time, we can see that three of the notes are still joining waiting a little longer and they're all up and normal. Okay, now we're going to run CQ l s H again on knowed one, and we'll create our plural side. Key space this time, however, will specify networked apology strategy and indicate that we want two copies of the data stored in data center D C one and only one copy stored in D. C too. Now the results of note tool describe bring plural site are a little more interesting. The data matching each token range will be stored in three places, but notice how those three places are allocated. If you look at each row of the results, you'll see that there are two locations in D. C. One and one in D. C, too. Additionally, it always chooses both rack one and Iraq to even though we have two nodes in rack one running no tool status. Plural sight again for this multi data center cluster gives some interesting results for the owns effective column. Starting from the bottom, it makes sense that this one note in Data Center D C two will own all the data. We've configured our key space to store one copy in D. C. To, and there's only one note there to receive the data. The allocations in D. C. One, though, are more intriguing. Are plural psyche Space will store two copies of its data in D. C. One. However, even though there are three nodes in D C one, they are allocated in only two racks. Therefore, Cassandra's first priority for this replication strategy is, to be sure to store each of the two copies in D. C. One on two different racks because there's only one note in rack to it will receive all the data as well. Finally, because there are two nodes and wreck one, the data there will be roughly split between those two notes. It's rare that you would have a configuration exactly like this in production, and in fact, it serves as a good example of why you should be careful when defining your cluster topology and your replication strategy. You'll want to avoid creating an imbalance within your data center, as we've done here. Finally, we'll wrap up the demo once again by cleaning up after ourselves

Tunable Consistency
[Autogenerated] Okay, so now we're going to take a look at how Cassandra Processes reads from and rights to the cluster. Like we've said from the beginning, each note in a Cassandra cluster can function in all the roles required of a cluster. However, within the scope of a single read or write, certain nodes will perform certain functions. For example, when a client connects to a cluster to perform an action, the note it connects to is called a coordinator node. For the duration of this action, this note will facilitate the communication between the client and the cluster. In this example, the coordinator node will receive the insert statement and using all the information we've covered so far, determined which nodes in the cluster should process the rights in order to store the data. It will then communicate this right to those nodes, which we call replica notes. Each interaction with Cassandra may originate with a different note, acting as the coordinator node, and there will be cases where this coordinator note is also one of the notes responsible for storing the data. One of the attributes that makes Cassandra stand out in the world of distributed no sequel databases. Is this notion of tunable consistency in a Cassandra cluster notes could become unresponsive for any number of reasons in a practical sense for rights to the database. This notion of consistency allows you to determine from how many nodes the coordinator will wait to receive an acknowledgement of the right before returning a positive response to the collar. Rather than setting that at a cluster key space or even table level, Cassandra allows you to determine the consistency with each statement. You can set this consistency level toe one, which means the coordinator will return success to the collar as soon as it receives an acknowledgment from just one of the replica notes. Two and three are the same before two and three nodes, respectively. Quorum is an important one. It means that success is reached when a majority of the nodes responsible for the data acknowledged the right all means. As you would expect that all replica notes must acknowledge the right inversely. This also means that if even one note responsible for storing the data is unavailable, the right will fail. Any is an interesting one. This consistency level means that if the right can make it into the cluster at all, even just to the coordinator note. It's considered a success, so we've talked about what gets returned to the collar under various levels of consistency. But what actually happens in the cluster itself, regardless of which right consistency you choose? Cassandra is going to write data toe all of the notes responsible for that data. However, what happens if one of those rights fails? If the right was performed with a consistency of quorum, the right will appear to have succeeded. Even if one of the notes is unavailable, Cassandra uses a strategy called a hinted handoff to help in these situations. The data is written down on the coordinator node. Then the coordinator know tries repeatedly to deliver the data to the note that was previously unavailable. Eventually, when the right is successful, the data is removed from the coordinator node. However, what would happen if, while one of the replica noses down, the coordinator note goes down, too? Now there's no one to inform the third replica note when it comes back online. Cassandra has a strategy for this at read time, and we'll see how it does it in just a minute consistency and reads works similar to rights. Read Consistency determines how many knows the coordinator will consult in an attempt to return the most current information to the caller. It does this by retrieving the data from one note and a digest of the data from additional notes. The exact number depends on the consistency level. Tunable read consistency allows the collar to control how important it is to receive the latest information in response to a query. The setting of one means that the data retrieved from one note will be sufficient to return to the caller. Two and three indicate that digests from one or two additional knows will be compared to the data prior to returning it to the caller. In a quorum consistency level, the data and digests from a majority of nodes will be consulted, and in all each note holding a copy of the data will be consulted to ensure the most up to date information is returned to the color. So what happens during this read process? If the replicas don't agree, let's go back to our earlier case where the coordinator node died and a hinted handoff was never processed. We now have one node with old data on it and two notes with good data. If you're reading with a consistency level of all, each node would be consulted. In this case, the third node will return a digest that does not match the other two nodes. The coordinator node will then read the full data from all three notes, determine the correct data and write it back to the no that's in error. Now all three nodes are back up to date. This is called read repair, since at a consistency level of all every replica is consulted, read repair happens as a part of returning the data to the caller for lower levels of consistency. A read repair is done for a portion of the reeds on an ongoing basis. For example, a read consistency of quorum in our earlier example could have simply consulted the 1st 2 nodes and returned that data to the caller. Then in the background, it would have double checked the third note and, having found it out of date, repaired the data because there's overhead. In this process, it's on. Lee, done for a percentage of lower consistency, reads and is configurable on a per table basis. Of course, this will only repair data that's actually being read. It's advisable that you periodically run the no tool repair command. Among other things, this will resolve inconsistencies like this among notes in your cluster without waiting for the data to be read. The full scope of no tool repair is outside the scope of this course. So with all this talk of consistency levels, hinted handoffs read repairs and the like. How do you actually achieve consistency? Cassandra has a reputation as an eventually consistent database, and it can be that if you want, however, it is possible to achieve strong consistency. With Cassandra. There is what's known as a magic formula for achieving strong consistency. If you're right, consistency and read consistency combined are greater than your replication factor. You have strong consistency. Let's walk through some examples of this. If you're writing with a consistency of one in reading with a quorum consistency level in your replication factor is three. You will have eventual consistency but not strong consistency. This is because for any given read, the quorum of no Jew consult could be the two nodes that have not yet been written to, and therefore you could be given old data for a right consistency of one in a reed consistency of all. You have strong consistency. If for some reason, the read from all encounters one new copy and two old copies of the data, it will perform a read repair, resolving the inconsistencies and update everyone with the latest data before returning it to the caller. Writing toe one and reading from one does not provide strong consistency. It's entirely possible you will read from a note other than the one that just received the right reading and writing from Quorum. A very common approach. Insurers that you were right to a majority of nodes and read from a majority of notes. This is a good way to achieve strong consistency. Okay, one last slide before we get to our demo. We've spent a lot of time talking about consistency, but we haven't really addressed it. In a multi data center scenario, there are three additional levels of consistency you can use when operating in a multi data center environment, a consistency level of each quorum. Insurers, a quorum of rights succeed in each data center prior to returning success. Local quorum, on the other hand, returns success once a quorum has been reached in the data center where the coordinator is located. These are important distinctions from a pure quorum consistency, which is applied without concern for where the nodes are located, which may mean your collar is waiting for a right to complete in a remote data center without realizing it. Finally, local one is the same as a consistency level of one, but on Lee attempts to read from the node in the data center where the coordinator is located. Thes consistency levels give you better control over when you cross the data center boundary on reads and writes, Okay, now we can finally get back to our demos.

Demo: Tunable Consistency
[Autogenerated] in this demo, we're going to try some different consistency examples and do so in both single and multi data center clusters. We'll bring up our three note single data center cluster by running Docker composed up dash D in our single D C directory. After a while, our cluster will be up and normal running CQ l s H. We create a key space with simple strategy and a replication factor of three. Next will use this key space and create our first table courses with one column I D. Which will be our primary key and by default are partition key as well. CQ l s H has a command to get and set the consistency level by running it here. We see that we're currently at a consistency level of one. We can insert a row into the table and it succeeds. No surprise here, setting the consistency level to quorum. We'll insert another course into the table and it to succeeds. Now let's set the consistency level to all. We'll use another CQ l s H command to turn on tracing. This will show us the steps Cassandra follows when processing the insert statement inserting 1/3 course into the table. We now get a lot of tracing out. Put back. Let's shrink our font so we can see a little bit more of this. Okay, there. That's better. Since we're signed into note in one, which is I p address 1 72.22 dot 0.2 that is acting as our coordinator. It determines which replicas need to process the insert and sends messages out to the other two notes. Now all three notes are simultaneously processing the insert statement when the note at I P 1 72.22 dot 0.4 has completed it in queues and sends a message back to the coordinator. Knowed 1 72 22 03 does the same thing When the coordinator has processed its own insert and received and processed the messages from the other two nodes, the request is complete. Now we're going to take one of the nodes in our cluster off line will run, Doctor Composed Stop in 32 Shut down that node. Now let's run CQ l s h again and using the plural psyche space will set the consistency level to all Now let's try to insert another course into our courses table. It fails, but that's okay. That's what we would have expected. The answer failed because it was unable to achieve our requested level of consistency. Now let's set our consistency level to quorum. Retrying the insert statement. We see that it now succeeds. No. Three is still down, but we no longer get an error because we're operating at a different level of consistency. However, in this case, the coordinator node has stored a hinted handoff in hopes that note in three will be back online soon. Selecting that row back out of the course table while still at a consistency level of Corum, we find the road just as we inserted it. If we set the consistency level back, toe all and rerun the select statement, we get a failure similar to our insert earlier. It cannot achieve the requested read consistency so no data is returned. Okay, let's exit CQ l s H and used docker composed to bring up our down note as this note comes online. Unbeknownst to us, he hinted handoff is happening in the background to inform note in three that we inserted some data while it was down. Let's run CQ l s h again and set the consistency level toe all using plural site and selecting our Cassandra developers course. We no longer see the error we saw previously. When note in three was down, We get back the row we inserted earlier. Since our consistency level was set to all, we can be sure that all three nodes agree that this is the correct data. It's a material tow us. Whether this is because a hinted handoff brought the third note up to date or a read repair was triggered because the reed happened before the hinted hand off could complete. All we need to know is that Cassandra took care of ensuring that we got what we needed at the consistency level we requested. Let's shut down our single data center cluster and get ready for the next part of our demo. Now, we'll look at some of the consistency levels associated with a multi data center cluster. We'll launch our multi data center cluster just as we did earlier in this module. Checking no tool status. No. Still starting up. Okay, once more. And we're good to go will run CQ l s H and create our plural site key space. This time we'll use network topology strategy and store three copies in D. C. One and just one copy in D. C, too. This is different than our last demo when we only stored two copies in D. C. One after we use plural site will create our courses table again. Our current consistency level is one, so we'll change it to local one. If we answered a course into our table, it succeeds, just as we would expect it to. Now let's exit CQ l s H and shut down our one note in D c to service in four. This would be similar to losing network count activity between data centers, the note or notes, and easy to would just simply disappear running no tool status. We see that Cassandra recognizes the node in D. C to as down. Now let's run secure LeSage again use plural site and set the consistency level to each quorum. This is going to require that we achieve a quorum in both D. C one and D. C, too. If we try to answer a row into the courses table. We get an error because our replication strategy requires that we store one copy in D. C to and because we want a quorum of nodes in each data center to successfully receive the right, we get an error. Now let's change this to local quorum and try our insert statement again. It succeeds. In many cases, local Corum is the preferred type of quorum when operating in a multi data center environment. Not only is a tolerant of connectivity errors between data centers, it also doesn't endure the Layton Sea of waiting for a response from one or many remote data centers before returning success to the caller. Finally, we'll shut down our cluster.

Conclusion
[Autogenerated] So if you want some extra credit, here's another demo. You can try for yourself. Launch a three note single data center cluster just like we've been doing. Used Docker compose execs to run CQ l s h and create a plural psyche space with a replication of simple strategy and a replication factor of three. Create a courses table in the plural psyche space with the same schema as in our demos exit Secure L s H and used docker Compose execs to run the note Tool pause, Handoff Command. This will stop hinted handoffs from being processed. Then used docker composed Stop to take down one of the nodes in your cluster. Now, with just two nodes running, go back into CQ l s H an insert one row into the courses table Next exit CQ l s H and used docker composed. Start to bring up the node. You just shut down Since hinted handoffs are paused, this new node will not automatically be brought up to date with the road that you just inserted. Run secure L s H and set The consistency to all this will force the reed repair tow happen as a part of your read request Set tracing toe on now run a select statement to retrieve the row you inserted above. Examine the tracing output and look for the digest mismatch. Exception. This is a sign that a read repair is necessary. That's it. So to wrap up this module, let's review We started off by introducing a few new Cassandra concepts and then immediately jumped into replication strategies, namely the simple strategy and the network apology strategy. From there, we looked at read and write consistency and the magic formula to achieve strong consistency along the way. We also looked at some of Cassandra's anti entropy strategies, hinted Handoff and Read Repair. I hope this module has been educational, and I encourage you to stick around for the next module as we transition to a more thorough introduction of sea Que el and the basics of Cassandra data modelling. Thank you

Introduction to CQL
Introducing CQL and cqlsh; Keyspaces, Tables and Common Datatypes
[Autogenerated] Hello. My name is Paul O'Fallon, and I'd like to welcome you to the next module in the course. Cassandra for developers, this module is entitled on introduction To seek you, L in this module will look at sea cules support for manipulating key spaces and tables, as well as the basic data types it supports. Then we'll see how to use C Q L to insert, update and delete information from Cassandra. Next we'll look. It counters A special data type supported by Cassandra Finally will wrap up this module by looking at the support for aggregate functions in Cassandra. Let's get started. Si que el or Cassandra Query Language is the preferred method for interacting with Cassandra. However, it hasn't always been this way. When it was introduced in 2008 the primary method for interacting with Cassandra was via a thrift, a P I. In 2011. Cassandra 0.8 brought the first iteration of the Cassandra Query language in 2012 CQ. L three was introduced, and this was further refined in 2013 when Cassandra to Dato introduced CQ l 3.1 and again when Cassandra Tudo one gave us CQ l 3.2. The version of Seek Well we're using in this course is 3.4 dot four. Each version of Sea Que El introduces new constructs for interacting with Cassandra while retiring some older ones. We no longer talk in terms of column families or super column families, as was the case in the earliest versions of Cassandra. Now our interaction model is defined in terms of key spaces, tables and rose. In fact, even though much of this course so far has dealt with partitions and where they are stored in the cluster, the term partition itself doesn't make an appearance in CQ l. It's a foundational concept, but partitions are manipulated through tables and rose. So far in this course, we've executed CQ l statements from the command line on several occasions. To do that, we've used a program called CQ L s H. While we've been invoking it within a docker container using its default parameters, there are many options you can pass into CQ l s H at the top. You'll notice that you can specify a host and port. The default which we've been using is to connect to a locally running Cassandra node when we executed in our doctor container. It's connecting to the node running in that container. However, you can also run CQ L S H locally and connected to a remote cluster by specifying the host and port number of a note in that cluster. If you wish to connect to this node using SSL, you can specify that with the SSL flag. If your cluster has user authentication enabled, you can specify those credentials here as well. If you are not interested in an interactive session, you can pass in a file of sea que el commands using dash F, and it will run those commands and exit in addition to the help option on the command line CQ l s H also has helped built right into the tool itself. Running this help command first shows us the commands provided by CQ l s H that aren't actually part of sea que el itself. You can run help and then any of these commands to learn more about what they do. For instance, we've been using consistency to set the consistency level of our reads and writes capture and copy can be used to export and in the case of copy import data into the database. DSC and describe are synonyms, and they are used to display information about the tables, keys, bases or the entire cluster source will execute a file of sea que el statements much like the dash F flag we discussed earlier. Tracing is another one we've already used to show us the steps Cassandra takes to resolve a C. Q L command. Finally, paging and expand control how the results of CQ L queries are written to the console. In addition to these CQ l S H on Lee commands, there are many help topics on CQ l itself. These are not one toe one with specific secure all commands and in some cases represent a hierarchy of information. Also, you'll see some help topics listed here for backwards compatibility purposes. The term column family shows up here several times. I encourage you to take some time to peruse the help topics, as we learned in an earlier module. Key spaces are the top level building blocks for organizing your data in Cassandra CQ l provides several commands for manipulating key spaces we've repeatedly executed that create key space statement. The only new option we see here is the durable rights option. Setting this defaults means that Cassandra will skip. It's commit log when writing data to tables in this key space, you most likely want to leave it at its default value of True as setting it defaults put you at risk of losing data. You can update an existing key space with the altar Key Space Command. Here you can change the durable rights setting and even the replication strategy. However, this change will require you to run the noted tool repair command on your cluster to reorganize the data according to the new strategy. Finally, you can remove a key space and all of its tables with the drop Key Space Command within a key space, we can create many different tables toe hold our data. The create table CQ l command does just what it says. Note that we are specifying the name of the key space as part of the table name. This is an alternative to the EU's statement you've seen in our demos. There are several ways you can alter an existing table. You can add a new column to a table with the altar table. Add command. You can also remove a column with the altar table dropped command. Additionally, there is some support for renaming or altering the data type of an existing column, but only in very limited cases. If you want to remove all the data from the table, you can use the trunk a command. And if you want to remove the table all together, you can use the drop table command when creating or altering a table. There are many different properties you can sit these are specified using with in a create table statement. Some of the properties you can specify for a table include a comment associated with the table cashing attributes of the table. Whether Cassandra should cash just the partition keys or a certain number of rows for each partition. The Reid repaired Chance and D C. Local read Repair Chance control the number of times Cassandra will run an a sink read repair process for select statements with reed consistencies that don't already require it. We covered read repair in an earlier module default. Time to live and G. C. Grace seconds relate to how Cassandra expires and removes data from the cluster. Something will look at in more detail shortly. In addition to these, there are many other properties you can set when creating a table, most of which allow you to tweak how the table data is stored on each node in the cluster When designing your table's columns. In Cassandra, you have many different data types to choose from. There are six different numeric data types, a mixture of inter jer and floating point of varying precision. Sze the each map to specific Java data types. Big int maps to a job along and decimal to a java dot math dot big decimal double float an end map to their job equivalents. Var inte maps to java dot math dot big energy er for holding strings. We have three data types. Ask e text and bar, car, text and bark are are synonyms and all three map to a job, a string. The only difference is that asking represents a US asking character string. While text and bark are represent a UTF eight encoded string to store dates, Cassandra has a time stamp data type. It also has a special data type toehold Type one. You you ideas called time U U I d. These allow you to leverage you your ideas that can be sorted based on date and time. They are a very useful data type in Cassandra, as we'll see in a later module. Finally, there's what's left a Boolean, a regular U U I D data type I net for storing I P addresses and blob. A blob data type provides a way to store arbitrary binary data. Whenever you're designing your key spaces or tables in Cassandra, there are some naming constraints to keep in mind. First, you can't use any hyphens in your names. Second, you can't use any spaces in your names, either. If you want to use a key space table or column name that starts with a number, you need to surround it in double quotes. The same is true for mixed case names. If you don't surround them and double quotes, they'll be flattened into lower case. The best rule of thumb I can give you is this. Don't get creative. Just keep it simple in our examples. So far, we've always seen a single column defined as the primary key of a table. This is the simplest case, and this primary key column is also used as the partition key for the table. Let's say, though, that in our data the idea of a course is on Lee unique for a given author. One way to represent this is to define both columns as the primary key. As shown here. The double parentheses are going to look weird right now, and that's okay. Stick with me. It will make sense in a coming module. In this case, the combination of I. D and author are used to create the partition key for the data inserted into this table. In both examples, however, were on Lee storing one Roper partition. This is true, but only for now, and only for primary keys defined this way. The structure of your primary key has a significant influence over how many rows of data are stored in a single partition of your table, and we'll learn more ways to influence this in a later module

Demo: Creating a Keyspace and a Multi-column Courses Table
[Autogenerated] in this demo, we're going to venture into CQ l s h. To create an altar, of course, is Table. So far in our demos, we've been launching multi node Cassandra clusters. However, to examine CQ l s h, we really only need a single note cluster and looking at the docker composed Jamel file for this module, you'll see it only has one no defined if memory constraints have kept you from the earlier demos, give this one a try. Running one cassandra note shouldn't take much memory. We'll launch it just like the others. With Docker composed up dash D no tool status shows were up and normal so we should be ready to run CQ l s h running help. You can see the help topics we reviewed earlier in the slides running help Consistency shows the help text for setting up the consistency level in your CQ l s H session. We used this in our last module. When demonstrating their various levels of read and write consistency running helped create table gives us a link to the create table documentation. It can't open the browser for us because we're running this in a docker container Okay, so let's start. Let's create our plural psyche space like we have so many times before. Since we're only running one Cassandra note. Let's just set the replication factor Toe one then will run used Plural site. This will keep us from having to prefix each of our table names with the plural site Key space. Now let's create a courses table using the same create table we've done several times before. Okay, great. Now look what happens if we try to create the same table again? Well, we get a KN error. One of the things Cassandra supports is an if not exists clause, which you can use when creating key spaces and tables. Okay, so we have a courses table, but it's just an I D. And that's not very interesting. Let's alter the table and add a duration field. We'll track the duration of a course in seconds, which weaken store as an ent. Next, let's add the date the course was released. We'll store that as a time stamp finally will add the author. As of our car altar table statements can also be used to set table properties, so let's add a comment to the table here. Now, if we run the DSC command to describe the courses table, we get a lot of information in return. At the top, we can see a complete create statement with all four columns, and if we look after the with Klaus, we can see our comment there as well. However, there are many more table properties here than those we specified. These represent the default values for all these properties which were set when we created the table. Now let's drop the table by running the drop table courses Command. Finally, let's create a complete courses table in one command CQ l S H supports multi line commands and it can determine when you're in the middle of a command and it will automatically indent the cursor and provide an ellipsis. Here. We're going to include all our earlier columns as well as a few new ones. Audience will hold an inter jer to represent beginner, intermediate or advanced, and CC will hold a Boolean to specify whether a course has closed. Captioning available. If we run DSC table courses again, we can see the complete table. This table will be at the center of our data design throughout the remainder of this course. As we learn, new concepts will expand and enhance this table. Toehold Maur and Maur information Before we wrap up, let's exit tic u l s h and shut down our one Cassandra note.

Selecting, Inserting, Updating and Deleting Data; TTLs and Tombstones
[Autogenerated] selecting data from a Cassandra table looks very similar to its relation. A liquid violent. Here we're selecting the idee and title columns from all the rose in our courses table. This type of query with know where Klaus limiting it to a specific partition should always be used with care. A query like this will cause the coordinator node to reach out to every other note in the cluster holding data for the courses table. Try running this query in CQ l s h with tracing on and you'll see what I mean. CQ l also supports aliases in select statements. Noticed, too, that now we've limited our query to a single partition by specifying the Cassandra Developers course. You can also use in as part of a wear clause to specify a series of values. This is another one to use with care, however, since there's no easy way to predict how many data nodes your coordinator will need to communicate with to reach all of the partitions referenced in the where Klaus Finally, you can also limit the results. Return from a query by specifying the limit clause. You can insert a row into Cassandra with the insert into statement. The comma separated list of column names is always required in a Cassandra insert statement. You can also update a row with the update statement using the set clause to specify the column names and values to update the where Klaus is required in an update statement. You can update multiple partitions at once, however, just like the select statement you can use in here and specify a comma separated list of identifying values, the same caveats around. Coordinator notes and data nodes apply here as well. It's important to note that on this slide, the 1st 2 statements are identical. Inserts and updates performed the exact same function in Cassandra. Either syntax can be used to create a new row or to add or update columns in an existing road. They're all effectively up. Certs. Cassandra has a special query function you can use if you want to determine the date and time a particular column value was written, the right time function will return you the UNIX time representation of when a piece of data was written to the database. As you might expect by now, you can remove a row from Cassandra with the delete from statement like the updates statement the where Klaus is required. You can also delete an individual column as we're doing here, by removing the author column Fromthe Cassandra Developers course deletes arm or than close cousins to insert and updates. They, too, are really the same thing. You can also remove a column by updating its value to know. In fact, you can even insert a Knoll column value into an existing row. All three of these statements are functionally the same. Cassandra has a way to delete data without you having to explicitly execute one of the previous commands. You can set a T T l or timeto live, and Cassandra will automatically remove the data when the time has expired. You can set the TT l for a single column value by supplying the using T TL clause in your updates. Statement. The value supplied should be the length of time in seconds. You want Cassandra toe hold onto this value. If you've previously set the t t l and want to retrieve it, you can do that with the T. T. L query function here. We're selecting the T T l value for a reset token. If you want to set the time to live for an entire row, you can do that with an insert statement has shown here. This is one case where the functionality of an insert an update diverge. If you want to delete an entire row once the T T L expires, you need to set it via an insert statement. Setting TT Els on one or more columns in an update statement will still leave a row. Even after all the columns have expired. There may be certain cases where you have more control over the structure of the table. Then you do the individual insert or updates statements. In that case, you can set a table wide T T l by specifying the default time to live table property at table create time. When data is deleted in Cassandra, it isn't just removed immediately from the cluster. A tombstone is written to the database, marking the data in question as deleted. This tombstone is partitioned data just like any other data written to the cluster, and this managed accordingly, with hinted handoffs, read repairs everything, say, For example, we have a key space with a replication factor of three, and we delete some data at a consistency level of quorum. Now let's suppose this right doesn't make it to the third node in a cluster responsible for that token range. Having the tombstone allows the reed repair process to propagate this tombstone to the outdated node, thereby bringing everyone back in sync. These tombstones are periodically purged from the database. However, it's important not to do this too frequently. Let's say we have the same scenario as before. Where to? Tombstone rights have succeeded in 1/3 1 is lost. We now have two nodes with tombstones and one with the data we originally wanted to delete. If the tombstones are purged from the database before the outdated note is brought back in sync, all we have left is one node with the data we originally hoped to delete. Now, when a read repair is triggered, guess what happens. Only one note has a seemingly valid copy of the data. So this data is sent back to the two other notes. The data we tried to delete has come back to life. This frequency of purging tombstones is called G C grace seconds. The number of seconds to wait between garbage collections and is a property you can specify on a per table basis. The default is 10 days.

Demo: Creating a Users Table; Using TTLs for Reset Tokens
[Autogenerated] in this demo will populate our courses table with the data from five plural site courses. Next, we'll watch the right time function. Return updated information as we change the data in our table. Finally, we'll add a user's table and leverage T t ells to manage password reset tokens. We'll start by re launching our single Cassandra Node. Next, we're going to load some course data. We'll do that by piping a file of CQ l commands into CQ l s H. You can scroll through the file and check it out if you like. It's a series of CQ l commands. If it's not obvious by now, we're going to use the demos in this course to build out a Cassandra Data model that a company like plural site might use as the underpinnings of its products as we go along, will collect our CQ l statements in a file and re import them into Cassandra like this. As we continue to evolve our data model running CQ l s H, we can use plural site and then run the DSC tables Command to see what we've imported. There's our courses table selecting star from this table. We see five courses we've imported from our courses dot c q l file. If we want to see the data without the line wrapping, we can run, expand on and execute the same select query again. Now we can see the results with each column on its own line. Next, we'll move on to the right time function. Let's look at the right time of this CC column for the advanced JAVASCRIPT course. Now let's update that value to faults and rerun the select statement, as we would expect. Since we've updated the value, the right time has also changed. Just to demonstrate that this is stored on a per column basis, let's select the right time of another column for the same course. You'll notice that this is the same right time as the CC column had before we updated it. Another interesting query function Cassandra provides is the token function. This returns the token value associated with the partition key. Thes token values can be mapped back to the output of the note tool ring and describe bring commands. We've run in earlier modules. You may have noticed that all of our query examples so far either include the I D. As part of the wear claws or do not have a where claws at all. If we try to query the courses table and supply an author in the where Klaus, we get an error later in the course will look at multiple ways to access data beyond just a partition key. But until then, we need to stick with this limitation to complement our courses table. Let's create a user's table. In our use case, these would be the users who can log into the website. Now let's insert one fake user named John Doe into the table. Let's try the update method of adding data to the table by upsetting a second user, Jane Doe. If we select star from the user's table, we see that the net effect is the same. Now let's assume we want to implement functionality on our website for resetting a lost password. As a part of this process, we'd likely want to generate a token to send as part of the password reset u R L. Let's alter our users table to add a column to store this password reset token, since we'd likely only want this token to be valid for a relatively short period of time. We should store this reset token in our users table with a timeto live value. Here we're updating the user's table to add a reset token for John Doe with a time to live of two minutes. This is actually pretty short. In reality, you'd likely set this to an hour or so, but a two minute detail makes it easier to demonstrate if we want to select the timeto live from the user's table. We can do that with the T. T L query function. Here we see we're already down to 105 seconds. Let's turn on tracing and run the select statement again. Pay attention to the zero tombstones sells row in the tracing results. Now let's run this select again. Still 74 seconds to go. Now, if we wait and run the select statement one more time, the reset token is gone and we have a tombstone. One of the reasons you want to garbage collect tombstones periodically is because they do take up space, and having many tombstones can affect your query performance. Since you must skip over these tombstones when finding and returning your query results. Finally, let's turn off tracing exit CQ l s h and shut down our Cassandra ____.

Counters and Aggregate Functions
[Autogenerated] the last data type we'll look at in this module is counters. Counters are a special data type in Cassandra, since dealing with them in a distributed environment requires special care. Counters must live in tables by themselves other than the primary keys they also carry with them. Some added overhead as they rely on read before, right. Semantics. Here we've created a table that has to counter columns to hold the ratings received. Four. Of course, the primary key is the course I D and R to counter columns are holding the total number of ratings and the sum of all the rating values. With these two pieces of information, we can divide one by the other and get the average rating for our course to increment a counter. We need to use an update statement here were updating both counter columns in the same statement. If we have just received a new rating of four for the Cassandra Developers course, we would store that by incriminating the number of ratings by one and adding four to the sum of all the ratings. This is okay, but there may be a better way to solve this problem. Let's look at another way to do this using aggregate functions, you can call these in your select statement to perform functions across a group of rose within a partition. They include Count Men, Max some and average for our ratings. We could create our table toe, hold the actual ratings instead of just a running count. This is nice, because now, if someone changes their course rating, it just updates the existing record rather than adding a new rating to the running totals. Also, with a table like this, we can use Cassandra's aggregate functions to return not only the average rating for, of course, but also the minimum, the maximum and the count of ratings as well. Let's take a look at both of these options in action.

Demo: Creating a Ratings Table Using Counters and Aggregate Functions
[Autogenerated] in this demo, we're going to create a table to hold course ratings. First, using counters following that will solve the same problem using a select statement with aggregate functions. Let's get started. We'll relaunch our single Cassandra Node with Docker composed up Dash D. Next, let's load some course data by piping our courses dot c q l file to seek u L S h. Once that's done will launch secure L S H and use plural site. Let's add a new table called ratings will specify the course I d. As our primary key and create our first counter column called Ratings Count and our second column called Ratings Total. Now we'll show the statement we would execute to add a new rating will execute an update, incriminating the ratings count by one and the ratings total by four. Now, if we run select from our ratings table, we see the data. We just absurd ID. Let's run another update to show adding another rating this time of three selecting again, we get what we expect. A rating count of two and a total of seven. Dividing one by the other gives us an average rating of three and 1/2 stars. Now let's try solving this. With aggregate functions, we'll drop our existing ratings table and create a new one with three columns to hold the course. The user and the user's rating of that course will define our primary key as the course I D and the user I D, which will keep all the ratings associated with a single course on the same partition. More on this in a later module. Next will insert some sample rating data. Nothing really special here. Now, with our sample date in our table, let's try querying. The average rating for a course seems pretty straightforward. Trying the same query for the Advanced Python course. We get an average of five, since there was only one rating for that course. One important caveat with these functions, however, unlike what you might expect, you can't apply them inquiries that span partitions here. If we were to select the course I D and average rating without specifying a where Klaus, we get really unusual results. The average is the average for all the courses, but the course I D is just one course, and the query is accompanied with a warning just to prove to ourselves. Let's query all the ratings from the table. These are the ratings that make up the 4.5 average above. So just remember, aggregate functions do what you expect within a single partition, but not across partitions. In fact, all this talk of within and across partitions is a good segue way into our next module on multi row partitions. Before we wrap up, let's exit tic u l s h and shut down our one Cassandra note.

Conclusion
[Autogenerated] So in conclusion, we started off this module by looking at how si que el has become the primary method for interacting with Cassandra. We then looked at the sea que el commands used for manipulating key spaces and tables. Following that, we took a quick tour of the basic data types supported by Cassandra, including numeric string date and other data types. After that, we looked at how to select, insert, update and delete data from Cassandra. When it comes to deleting, we looked at setting the time to live for your data and the tombstones that are created when data is removed. We wrapped up by looking at counters and aggregate functions. I hope this module has been a helpful introduction to seek ul and that you'll stick around for our next module. Thank you.

Multi-row Partitions
Composite Partition Keys and Clustering Keys
[Autogenerated] Hello. My name is Paul O'Fallon, and I'd like to welcome you to the next module in the course. Cassandra four Developers. This module is entitled Multi row partitions in this module will take a look at building tables with composite primary keys using static columns and a couple of different strategies for handling time Series data in Cassandra. Let's get started. So far, our tables have been limited to a single primary key. We have to find them this way by adding the term primary key to the right of the column in the table. CQ l definition. This is functionally equivalent to including a separate primary key designation within the create table statement. In either case, the net effect is that each partition contains only one row of data. There are several different ways to define the primary key of a Cassandra table. First, there's the one we were just looking at on the previous slide. A single call in this case, the primary key is made up of just the partition key. However, you can define a primary key using more than one column. The first column in the primary key is still the partition key, but Now we have a second column, which is referred to as a clustering key. Together, these make up a composite primary key. With this configuration, we are now no longer constrained to one roper partition. A table with a composite key can contain multiple rows stored together in a single partition. In fact, we can take this even further. You are not limited to a composite primary key made up of only one partition key and one clustering key. You can have multiple clustering keys. These additional clustering keys are simply appended to the end of the primary key definition. In the CQ, l create table statement. While we're at it, though, let's not stop there. Cassandra also supports specifying multiple columns to make up the partition key. These columns are used together to define the token Cassandra relies on when storing or retrieving data from the cluster. In our final example, if we take away the clustering keys were left with the double parentheses example that showed up in an earlier module. In this case, we have multiple columns that make up the partition key, but no clustering key. This mail still seem a little confusing, but we'll take a look at several concrete examples throughout this module. So far, in our courses table, we've been storing data about just the course itself. However, a course is comprised of multiple modules. It would be great if we could store the information about a courses modules along with the course itself. Since Cassandra doesn't support joins, we must look at other strategies for relating course and module information together, let's see what this might look like in our courses table. So far, we've had a few columns in a single primary key made up of the courses I D. Now we're going to add a couple of additional columns, toe hold information about each module within the course, and then we're going to change our primary key. Now our primary key is made up of both of the idea of the course and the idea of the module. So how is that gonna work? Well, for starters, here's how we'll need to insert data into the new courses table. Notice that now we're inserting both data about the course as well as data about the first module. Next to answer data about the second module will do a very similar insert statement repeating the course level data but replacing the module level information to reflect Module two to retrieve data from our new schema. The simplest select statement still looks the same as before, however. Now we can also add an additional column to the where clause in our select statement, the module I d. This is because the module I D is part of the primary key. Not only can we use it as part of the where Klaus, but we can also sort the data within a single course by module i d. Both filtering and sorting are somewhat constrained in Cassandra and really hinge on the columns and their relationship to the tables. Primary key. We still can't use an arbitrary column like author or module name in these scenarios yet, but we do now have that option for a module I D. Also, while these are all queer ing a single partition in our courses table, they return a varying number of rows. The 1st 1 returns to rose the second one row, and the third statement returns to rose further proof that we've moved beyond the constraints of one roper partition. This clustering key not only impacts how the data can be accessed but how it is stored in the cluster, all the data for a single partition. KIIS still stored it together. In our example, that is the I d of the course. As Rose are inserted into the table, they are stored with their partition, and the columns associated with each row are stored it together or clustered by the clustering key. When we insert the data associated with our first module, it is stored in the advanced python partition, and the non primary key columns are stored it together under the clustering key of module I D equals one. When the second row is inserted, it too is stored in the same partition, and its columns are stored it together under the module I d equals to clustering key. Let's stop here and take a look at a demonstration of this concept.

Demo: Composite Partition Keys and Clustering Keys
[Autogenerated] in this demo, we'll add support for modules to our courses. Schema will then leverage are clustering key to select module specific data from our table. Let's get started. First, we'll start our single Cassandra node with Docker composed up Dash D Waiting a minute or so. If we check on our node with node tool status, we see that the notice up and normal. Now we're going to load some course data from a previous module by piping these CQ l commands to seek u l s h. With that done, we can run CQ l s H use the plural site key space and describe our courses table. You'll see here that we have the same seven columns we ended up with in the last module. Doing a select star from the courses table shows the five rows of sample data. We added as well. Now we're going to drop this version of the courses table in order to re create it using our composite primary key. This create table statement contains the same seven columns as before, as well as three new ones. Module I D module name and module duration toe Hold the length for a specific module with the table created will insert a couple of rows of data for the course know Js the big picture. Now, if we stop into a select star from courses, we can see that each row is really a specific module with the course information repeated on each row alongside it. That makes sense, since that's basically how we inserted it, adding the course i d to the where Klaus returns to the same information. Now let's try adding a second module I d parameter to the where Klaus we get back. Just the row for Module two, the course data plus the individual module data. Now let's try to select star from courses and just specify the module. I d. Not the course I d Here. You'll see. We get a warning back from Cassandra, Since Cassandra will need to retrieve all the rose from the table across partitions and apply the module I d filtering afterwards, it wants to know whether we really want to do that. If we add, allow filtering to the end of the query, we see that we get back what we would expect. Note that you should be careful and not just arbitrarily add. Allow filtering. Whenever you see this error, there could be significant performance implications, especially if the where Klaus will only match a small number of rows Cassandra will still need to fetch and subsequently throw away. Ah, large number of rows. They're almost always better ways to accomplish this as we'll see in a later module. Okay, now let's add the remaining modules for this course. Not only can we use our clustering column in a where Klaus, we can also use it to Forman in Klaus. Clearing across modules within a course doesn't carry the same concerns as we discussed earlier When clearing for multiple courses across partition keys. These queries are all staying within a single partition. Now let's run another select statement, but order the results by module I d. In descending order. So, up till now, we've been using the courses table to store and retrieve data about a course. Can we still get just that data back out of our new table? Let's see, we can select distinct i d from the table and we'll get back our i d. However, if we want just our i d and name back, we can try that query, but it won't work. As you can see from the error here, maybe static columns will help. We'll take a look at those next.

Static Columns
[Autogenerated] static columns are a feature in Cassandra that allow you to specify columns whose values are constant for all rose within a partition. You can specify this by a pending the keyword static to the end of a column definition, as we've done for the name field here. Simple enough. But what are the implications of this? Earlier in this module, when we introduced composite keys, each row in our new courses table was storing data for both a module as well as the entire course. This was problematic because we needed to insert the course data with every row, and Cassandra was in turn storing a separate copy of the course data each time. Static columns make this scenario much nicer. Now when we insert a new row for a module, we only need to specify the data associated with that module. Our static a column is associate ID with the partition itself and not anyone. Row or module static columns give us flexibility and how we add data to our table. If we want to insert the course specific data, we can do that with an insert statement like this, just specifying the primary key course I D. and the static field name. Now, when it comes time to insert a module, we can focus on that. We still specify the partition key Advanced Python, but everything else is specific to the module. We could do the same with an update, just said the name of the module and specified the partition and clustering keys in the wear clogs. That's much nicer. Even more so is that our select statements don't have to change if we don't want them to. We can still select the course and module data together as if they were all stored it together. But now we can do even more. Let's take a look at this in action.

Demo: Static Columns
[Autogenerated] in this demo will update our courses schema to convert course wide fields into static columns will then show how this new approach makes it easy to query both course and module level data. Let's get started. First, we'll start our single Cassandra node with Dr Composed Up Dash D waiting a minute or so. If we check on our node with no tool status, we see that the notice up and normal. Now we're going to load some course data from our previous module by piping these CQ all commands to seek u l s h. With that done, we can run CQ l s H used the plural psyche space and drop our existing courses table. We'll create a new one this time. But now specify that all the course related fields are static name, author, audience and the rest will leave the module related fields as regular columns module I D module name and module duration for our first insert will focus on the course. Specific static columns for the values will insert all the same values as before. Notice that in this insert where including only a portion of our primary key, we're supplying the partition key of know Js big picture, but no clustering key. That's because all the non key columns here are static columns. If we select star to retrieve what we inserted, we see that we get Nell's back for the module columns. Those air still columns in the table, and since we didn't supply any values for them, they come back. No. Now let's insert data for our 1st 2 modules. Noticed that here we only need to specify the partition key I. D column and the three module columns. We're not inserting any course data, just module data. If we do a select star from courses, what do we see? It looks just like our last demo. We see all the columns populated for each row, even though we only inserted the course data once before any of our modules. It's returned alongside each module, just like before. And just like before, we can use the module I d. As part of the wear Klaus selecting on Lee the row for Module two. Now let's insert Module three, but set one of the static columns at the same time. We'll insert a different course Dame, which is one of the static columns for the course. Now, if we do a select star from courses, what do we see? The name of the course has changed for all the resin. The results not just module three. That's because the data in the static columns are associated with all the rose. In that partition, let's insert data for modules four and five and fix the name of the course. While we're at it, we can use the same in claws as we did last time. The results look identical. This is also true if we specify the sword order. If we try to select just the static columns, we get back one for every row. Which makes sense, since the number of rows in the table is determined by how many primary key or partition plus clustering key combinations we have now, let's try the select distinct that didn't work last time will select distinct all the static columns and see what we get. That's great. We can now interact with the table at a course, is level and retrieve just one row. Describing the course. We can still interact with the table as a series of modules. However, if we select the three module columns, we get a nice set of data telling us about the modules in the course. Let's see what this looks like with our five sample courses to do that. Well, exit CQ l s H and import a file from the M four directory. With that done, let's rerun CQ l s H and use plural site. Okay, Now let's select the modules for our advanced JavaScript course. Let's try it for Advanced Python, too. Finally will run our select distinct query to return just the course information for all five of our courses. Great. This is just one example of using static columns. We'll see another one as we venture into the world of time series data that's coming up next to wrap up this demo. Let's exit secure L s H and shut down our Cassandra node.

Time Series Data
[Autogenerated] time. Siri's data is an area where Cassandra really shines time. Siri's data comes in many forms, and from many sources click stream data from a website would be one example of times. Here is data. The rise of the Internet of things in the home is another. Data from sensors or machinery in an industrial setting is another emerging opportunity for collecting time. Siri's data one element that Cassandra brings to the table to help with this is the time you you i d. Data type. Here is an example of a time you your i d, which is really just a version one u u i d this type of u U I. D is comprised of three pieces of information. The number of 100 Dan a second intervals since the U. U I d epoch, the Mac address of the machine generating the you your I D and a clock sequence number designed to prevent duplicates and Cassandra time u u I. D s have a two fold benefit then, then they are both guaranteed, unique and sore double by the date and time imbedded within them. For our use case, let's take a look at a feature implemented by the website Mashable. This tiny embedded graphic shows the velocity of people sharing this article on social media. Let's say we wanted to create something similar to show the popularity of a course over time on plural sites website. Maybe we want to show how often a courses webpage has been accessed and add this graphic to the courses page itself. For starters, we'll need to record all the times a course page is accessed. To do that will create a table called course page views. It only has two columns. Course I D, which is the idea of the course whose page was viewed. Interview i D, Which will be a time you you i d made up of the date and time the page was viewed. These two columns together make up the primary key with the course I d. As the partition key and the view i d. As the clustering key noticed, too, that we're specifying a clustering order. This will tell Cassandra to store the data in each partition ordered by the clustering key in descending order. Si que el provides several functions for using time. U u I D s the first wheel cover is the now function. This function returns a time you your I D. Value with a current time stamp as its time component. This is handy to embed in an insert statement when you don't necessarily need to know the value that's being used, just that it's unique and associated with the current date and time. When selecting data from a table that contains a time, you you write a column secure. All provides a couple of functions for extracting the date and time component of the year. I D. Date of will return the date and time associated with the time you your i D Value UNIX time stamp of will return the date and time in a UNIX timestamp format. CQ. L also supports selecting Rose based on a date range of time. U u I D. Values. To accomplish this, it provides two functions. Men time. U u I D and Max Time. You your i d. These allow you to specify a date and time to match time. You your i. D. S that fall before or after this time. In this example, our query will match course views that happened during the month of November 2019 using a time you you I D. As a clustering key is a very effective method for handling time. Siri's data. For starters, their uniqueness is a benefit, as users view course pages are Cluster will receive a tremendous number of rights originating from many different coordinator notes around the cluster, with a replication factor of three and a right consistency of one. It's expected that these rights will arrive on each of the nodes at different times. Some even happening after reed repairs or hinted handoffs. Having a guaranteed, unique value ensures that all of these scattered rights don't inadvertently clobber one another, even when two rights happened at exactly the same time in the cluster. Another benefit leverage is how Cassandra stores data in a table defined with a clustering key. The partition data is ordered on disk by the clustering key, regardless of when the data arrives on each node. For example, let's say we answered a row with a view I d, starting with two D zero, as shown here since we defined our table with a clustering order of descending. When the next row is inserted with a more recent time. U U I d the new row is stored before the original Roe. However, let's say that there was actually another view which really happened during the time in between these two views, but for some reason hasn't made it to this note yet. When that date arrives, it will be added in the correct spot based on the time order of the time You you I devalue querying Rose from this table will always return the rose in time order regardless of the order in which they arrived on the node.

Demo: Time Series Data
[Autogenerated] in this demo will create a table toe, hold our course page views. Using that table will insert page views that contain a time you your i d as well as a timeto live t t l value finally will extend our table with a static column to track the last time a page was viewed. Let's get started. First, we'll start our single Cassandra node with Docker Compose up dash D waiting a minute or so if we check our node with no tool status, we see that the notice up and normal. Now we're going to load some course data from our current module by piping these CQ l commands to seek u l s h. With that done, we can run CQ l s H and use the plural site key space. Next, we'll create a course page views table will define course I d As of our car and view i d as a time you you i d We'll set the primary key to be these two columns and defined the clustering order as descending on view I d. I didn't mention this before, but it's worth noting that in this table we have nothing but our primary key columns. That's okay and not uncommon. Next we'll make our first insert into the table, will define our two columns and insert the values know Js big picture and use the time you your i d now function to generate a time you your i d using the current date and time. We're also going to add something here that we covered in honor earlier module. We're going to add a T t l to r insert statement. Setting a T t l to a year means that our table will automatically clean up after itself so we'll only have page views that are under a year old. We'll do a second insert, but this time we're going to specify a time you i d manually. This is also acceptable if you're writing code to do the insert and congenital rate, the u U I. D. There you may choose this option, for example, if you need to know the u U I d in your client code in order to use it somewhere else. Finally, we'll insert a couple of additional page views for this course. Great. Now let's run a select star from our table. Well, it seems reasonable. Just what we would expect Notice, though, that the insert we made with the manually generated time u u I d is the last row returned. This is because the date associated with that time you you i d is in the past. And since Cassandra is storing these rows in descending order by view i d, this row is stored and returned last. Now let's try using the date of function to extract the date time portion of the time you you i d values interesting. Now we can really see the age on that manually entered time u u i d It was from two months ago. Next, let's specify an order by in our query and get the dates back in a sending order. Great. Now let's try our men and Max time u u I D functions will select the date of a view i d from our table where the course I d is no j s big picture And the view i d has a daytime component on or after October 30th. But before November 2nd, this returns on Lee the row with the time you you i d we entered by hand containing a date of October 30th being able to extract in limit based on the date as an extra dimension to the benefits of a time. You you i d if what you really want is a date value, but you need to ensure that the inserts are unique. Time. U u I d is for you. Before we wrap up this demo, we're going to bring in another concept from earlier in this module. But let's trunk at our table and clear out the data. First, let's say we not only want to store all the views in our table, but we also want a quick way to get access to the most recent course view. We might use this data to periodically check for new course views and refresh the embedded graphic displayed in our course Web pages. One way to solve this is to add another column to our table. A static column called Last View i D, which will also hold a time you your i d. Now let's insert some rose into this new table before our time. You, your I d. S will just use the now function twice once for the last view i D. And another for our original view I D column. Now let's select from our table notice that in our results, our view I d is different for each row like we would expect. However, our third column contains the last view i d. The most recent view i d for this course. Since it's a static column, there is only one copy of it's stored for the entire partition. Updating it every time we do an insert is just a easy way to make sure that it always has the most recent view I d neat. If we select the distinct course I D and last view I d from the table, we get just the course I d in an i d. From the most recent time it was viewed. Of course, if we were only concerned about the most recent view for a single course, we could just execute a query like this. Select the course i d. In the view I d in at a limit of one, since we're ordering our views in descending order. This, too, is the latest view. However, let's assume we have more than one course let's insert a few views into the table for the advanced JavaScript course. If we rerun our same select with a limit of one, we, of course, get our original row back. We can run this same query for our advanced JavaScript course, but there's no way, at least not yet to Onley retrieve the first row of each partition from multiple partitions. If we run the query, select distinct course I D and last of you i d from our table. We get back one row for each partition, which contains our partition key, the course I D and the static last of you i d column. However, we still have access to the individual views on a per course spaces, which we can get if we still like just the course I d. In view i d from our courses table for the know Js big picture. Of course, the same is true. If we run the same query for our advanced JavaScript course, let's rub up this demo by exiting CQ l s H and shutting down our Cassandra node

Bucketing Time Series Data
[Autogenerated] In our last example, we used a T t. L. When inserting data into our course page views table. This puts some boundary on the data that would be stored in each partition. In Cassandra, there are some limits on the size of a partition. It can hold a maximum of two billion cells where a cell is a column in a row. Also, all the data associated with the partition must fit on a single note in the cluster. When storing time. Siri's data you must plan for what happens to your storage overtime. If we had continued to insert into our table without a T t l and just left it running for years and years, it's possible that we would eventually exceeded the bounds of a single partition. Well, maybe not for our use cases specifically, but if we were tracking sensor data that reported at very short intervals or storing large amounts of data in each row, it could certainly happen. There may also be cases where a T t. L is not acceptable. We don't want to throw away any data. We want to keep it all. A strategy for addressing this is to bucket your time. Siri's data In this use case, we want to store all the page views for a course, not just a year's worth to help us with this. We're going to add a new column to our course page for use table bucket. I d were also going to add this column to our primary key as part of our partition key. This means that as page views are received, they will be assigned to a partition based on both the course I D and the value of the bucket I d. Inserting the data will look pretty familiar, but we'll need to add a bucket i d. Here in this case, were bucket ing data by month, providing a bucket I d representing December 2019 as Rose for the Advanced Python course are inserted with this bucket I d. They'll go into the same partition. However, once we begin inserting page views for January 2020 the data will be written to a new partition. The art of this approach comes in deciding what to use as the bucket I d. It's a balance between how much data you expect to receive over a given time period and how you plan to retrieve it

Demo: Bucketing Time Series Data
[Autogenerated] So in conclusion, in this module we introduced clustering keys and composite primary keys as a way for Cassandra to storm or than one row in a partition. We also looked at static columns as a way to pin data to an entire partition across all the rose in that partition. We then introduced the concept of time Siri's data and showed how the time you you I d data type is perfectly aligned to this type of problem. We wrapped up by looking at the different strategies for managing the storage of time series data, including T, T ells and bucket ing. I hope this module has been helpful. Stick around as we dive into Cassandra's support for complex data types in the next module. Thank you.

Conclusion
[Autogenerated] So in conclusion, in this module we introduced clustering keys and composite primary keys as a way for Cassandra to storm or than one row in a partition. We also looked at static columns as a way to pin data to an entire partition across all the rose in that partition. We then introduced the concept of time Siri's data and showed how the time you you I d data type is perfectly aligned to this type of problem. We wrapped up by looking at the different strategies for managing the storage of time series data, including T, T ells and bucket ing. I hope this module has been helpful. Stick around as we dive into Cassandra's support for complex data types in the next module. Thank you.

Complex Data Types
Collections: Sets and Lists
[Autogenerated] Hello. My name is Paul O'Fallon, and I'd like to welcome you to the next module in the course. Cassandra. For developers, this module is entitled complex data types. In this module, we're going to cover several data types Cassandra offers. They go above and beyond the simple data types we've covered so far. We'll talk about collections, including sets, lists and maps, and then we'll look at two poles. Next, we'll take a look at user defined types will wrap up with a brief discussion on using Jason for reading and writing data. Let's get started to start off our view of collections. Let's look at Cassandra's support for sets. In this example, we go back to our original CC field in the courses table. If we look at the way this is displayed on the Web page, closed captioning is really just one of many features that a course may have. It may make more sense for us to store this as just one of many possible features associated with the course. Since we really don't want toe add columns to our table every time a new feature is introduced, let's create a set in the courses table toe hold the features. This would be a set of of our cars or strings and would look like this. Note that collections and Cassandra are very similar to their counterparts in strongly typed language is all the entries in the set must be of the same data type of our car. In this case, Si que el makes manipulating sets very straightforward. To insert a new row of data that contains a set, you simply provide all the scent elements enclosed in curly braces. To add one or more elements to a set in an existing row, you can use the plus sign to add the elements to the set. Likewise, removing elements from an existing set is just a simple using the minus sign. You can update a row to remove one or more elements from a set. Finally, if you want to update a row and empty the entire set, you can set the column to an empty set of curly braces. Next, we'll move on to the second type of collection. We're going to cover the list in our courses. Example. We know that each course is made up of modules. However, each module is also made up of one or more clips. These clips are the actual snippets of video that, when combined, make up a module and then the course. For now, let's just presume we want to store the names of the clips associated with each module in. Of course, we can do that by creating a list of of our cars. We want to use a list instead of a set in this case, because maintaining the order of the clips is important. Using CQ L to manipulate a list is even more powerful than manipulating a set. We can still insert a new row of data with a list. The only difference here is that we put the list in square brackets instead of curly braces. There are a few different ways to add elements to a list. Two of these use the plus sign. We can pre penned elements to an existing list by putting them before the plus sign in the name of the column after the plus sign. Similarly, we can upend elements to an existing list by reversing the order and putting the new value after the plus sign. It seems simple enough. One way to remove elements from a list is to use the minus sign. Subtracting elements from a list like this will remove all the matching elements from the list. This means that even if the same string appears on multiple times in the list, they will all be removed. Not only can you manipulate a list by adding and subtracting elements, but you can also manipulate them by their position in the list. You can update a single element in a list by referring to its position as you might in Honore. You can also remove individual items from the list the same way.

Demo: Sets and Lists
[Autogenerated] in this demo, we'll add the features set and the list of clips to our courses. Schema. Well, then look at adding and subtracting values from these collection fields. Let's get started. We'll start by launching our single note Cassandra Cluster, as we have many times before. Once it's up and running will load the courses data from our previous module. Next will run secure Ellis Age, Use plural site and take a look at the courses table schema as it stands now. Okay, great. Now we'll add our features set by first dropping the CC column from our table. Then we'll add a features column that will be a set of our cars. And since there's only one set of features for the entire course will make it static. Now let's add the CC feature back to our know Js big picture course with a nup date statement running select distinct and pulling back. The features shows are one cc feature for just the know Js course. Next, just for demonstration purposes, let's add a couple of extra fake features to the node course. We'll use the addition approach and add features F one and F too no to that here were adding multiple elements to a set. At the same time, you're not limited to just one now. Selecting distinct again, we see our three features for the one course. Let's also used the subtraction approach and remove our two fake features selecting again, and we see that they're gone. Now let's move on. From a set to a list will alter our course is Table to add a Clips column, which is a list of our cars to hold the name of each clip in order. With that in place, we'll add the only clip in the first module of the node course. Selecting back, we see what we would expect next. Let's add the first clip in Module two. Selecting again. Great. Now let's change the name of that clip with a nup date statement referring to the element by its position in the list. Hey, we got an error. This is because lists and Cassandra started Index zero, not one. Let's try that update statement again with an index of zero Okay, no error this time selecting again, and we see the updated clip name for another demonstration will add two more fake clips to the end of this list of clips will call them both clip and will add them in the same statement, similar to our set example earlier. Now, if we select back, we see four clips, including our two fake ones. Now let's remove our demonstration clips from the list. Notice that we're using the subtraction approach, but only including one clip in the list of clips to remove. Since this approach matches by name, it will remove all the elements in the list which matched that name. If we select back to check this, we see that both clipping trees are gone and we're left with our to clip injuries for the first node course modules. We'll wrap up this demo by exiting CQ l s H and shutting down our Cassandra node.

Collections: Maps, TTLs
[Autogenerated] will round out our discussion of collections by looking at maps. A plural site user may log in to their account on many different platforms. They could log in from their computer or on their mobile device, or maybe even access it from their game system. We would like to warn a user when they're signing in from a new device for the first time. To do this, it would be helpful to save the last time a user logged in from each of their devices. One way to accomplish this is to extend our users table by adding a last log in column. This column would be a map with var car keys and time stamp values. The keys would be fingerprints. Identifying each device and the values would represent the most recent data in time. The user logged in with that device. Inserting a new road with a map is similar to both sets and lists. In the case of a map, the format looks like a Jason object. Each key and value are separated by a colon, and multiple key value pairs are separated by commas. You can add to an existing map a couple of different ways. You can set the value by referencing a specific key in the map similar to its programming language counterparts. However, to be consistent with sets and lists, you could also now add a new key value pair to a map using the plus sign. Removing elements from a map also follows the same two approaches. You can delete a key value pair by referencing that specific key in the map. You can also subtract an element from a map using its key. Finally, you can remove an entire map by setting the column to empty curly braces. A really nice feature of collections is that T. T. Ells work against the individual elements of a collection. That means we can add Loggins to our map and set the tea tale to a year. Then, ah, log in from that device that we haven't seen in over a year will be treated as a new log in

Demo: Maps, TTLs
[Autogenerated] in this demo will update our users table with a map to store a user's last log in from each of their devices. We'll also demonstrate, adding this last log in information with a T T l and having it expire. Let's get started. We'll start this demo like we have so many others by launching our single Cassandra Node with Dr Compose. Once it's up and running, will load User's data from an earlier module. Once that is complete, will run secure L S H Use plural site and take a look at the user's table we just imported. Let's update this table by adding a last log in map toe. Hold the device fingerprints with their last log in date and time. Now we'll update the table, adding an entry toothy last log in map for user John Doe Here we're using the addition approach, as we did with sets and lists in the last demo selecting back Well, we see what we expect. Next, we'll add one more injury to the map, but this time we'll do two things differently. First, we'll set a t t. L of 60 seconds for this map entry. Second will add this one by setting the value in a fashion similar to how you might set a map entry in a programming language. Now, if we select the map of last log in from users for John Doe, we see our two map entries. If we wait a minute before selecting again the key with the TT LF 60 seconds is gone great. We'll wrap up this demo by exiting CQ l s H and shutting down our Cassandra node.

Tuples, Nested Types
[Autogenerated] another complex data types supported by Cassandra is tthe E to pull a tupelo is a series of values where those values may be of different data types but are expected to appear in a certain order. Here we have a tube all that can contain five elements to our cars, two ends and the time stamp. But in the exact order you see here, in our example, let's say we want to store more than just the last time a user logged in with each device. We would also like to store the I P address used during that session. We can accomplish this by introducing A to pull in, adding the to pull to our users table were actually introducing a couple of new concepts. First is the to pull data type itself. Notice we have a to pull with time stamp and I net elements thes will hold our last log in Dayton Time and I P address, respectively. However, this is also a demonstration of nesting complex data types are to pull here is actually the value of our last log in map. When you nest these complex data types, you must do so with the frozen keyword frozen is required because today nested data types are stored as a single blob value in the underlying data store. This is true even if you're nesting to collections together, such as a list of sets because they're stored as blob values, they must be read and set as a whole. One nice thing about having to include this key word, however, is that it makes this constraint obvious. And since Cassandra is evolving rapidly, it always leaves open the option for removing this constraint in the future.

Demo: Tuples, Nested Types
[Autogenerated] in this demo will update our users table so that we can store the I P address along with the date and time for each last log in map entry. We'll do this using the to pull data type. Let's get started. We'll bring up our Cassandra Node with Dr Compose Up Dash D. Then, when it's up and running, we'll load our user data from a Nurlita module. With that in place will run secure Ellis Age and use plural site. Let's update last log in in our user table to be a map with a key of our car. The device fingerprint and the value of A to pull this to bowl will contain a time stamp, the last log in Dayton time and an I P address represented here with the Annette data type. Now we can update our users table. Adding a new entry to the last log in map will provide a device fingerprint, a last log in dating time and an I P address selecting John Doe's last Loggins. We see what we just inserted, which is exactly what we would expect. Finally will exit secure L s H and shut down our Cassandra Newt

User Defined Types, JSON
[Autogenerated] we'll wrap up this tour of complex data types by looking at the most flexible of them all. The user defined data type in our scenario will use the user defined data type to fully describe the list of clips we introduced earlier in the module. Si que el gives us a way to create a new type with the create type statement. Here we're creating a type called Clip with a name, property of type of our car and a duration property of type end, allowing us to store two pieces of information about each clip. Now, with this type created, we can use it anywhere. We would use a regular Cassandra Data type. In our example. We're going to use this as a nested type inside of our list of clips. Now, instead of a list of our cars are clips will be a list of elements of the type clip. Also notice that we're still using the frozen key word here, so why not just use a to pull? As you can see from the clip data type with a user to find type, we can identify individual elements by name, and not just by their position in the to pull. This is especially helpful when you have a multiple elements of the same data type. A tube full of three in triggers, for example, doesn't do much to explain what each entered your actually represents. That being said, it can be helpful to start modeling with A to pull first and then create your user to find type once you've settled on a list of properties. Another nice thing about a user to find type is that they can be reused across multiple tables within a key space. A fairly standard type like address could show up in multiple tables. Here's another example of a user defined type, but one that's not being nested, because when we render a course Web page, we may want to link to the author's Paige. We need both an author's name as well as their I D. Our person type will allow us to store both values. Together, we can use this person type to replace the author column. Note that we still need the frozen keyword here, even though it's not nested all user to find data types currently require the frozen keyword. It wouldn't surprise me at all to see this constraint removed in a future Cassandra release. Having introduced so many complex, nested data types, it seems appropriate to take a minute and explore how we can leverage Jason to make dealing with these complex structures a little easier. For example, here's what it looks like inserting data into our courses table. The regular way on the right is Cassandra's support for the Jason keyword. With this, we can provide the same data formatted as a Jason object, and Cassandra will parse it and handle the insert accordingly. Note that it's not storing the data in Jason format, just allowing you to form at the insert statement. This way, you can also select data from Cassandra in Jason format and even specify individual fields to retrieve as Jason.

Demo: User Defined Types, JSON
[Autogenerated] in this demo, we're going to create a user defined data type to hold our modules, clip information before we're done. We'll also try selecting our course data as Jason. We'll start by launching our Cassandra node loading module data from a earlier course running CQ L s H and using plural site will create the clip type with two properties. A name, var car property at a duration entered your property. Next, we'll add the clips property to our courses table as a list of these clip types. Okay, lets try inserting a clip into our courses table. Great. Now let's out another clip with an update statement. Nice selecting back the list of clips for this course and we see the two we just added neat now, but exit secure. L s H and load the courses Data from this modules directory. Note the M five in the path. This contains the full list of all the modules and clips from the five courses we've been using throughout this course. Now let's run CQ l s H use plural site and select the clips from the advanced Python course. That's a lot of clips. Now let's do the same for the reactor. Big picture course. I encourage you to take a few minutes and try some of the other select statements we've covered in the course. There's a lot of data to work with here before we wrap up. Let's try selecting one courses. Clip data as Jason. While the select statement does return each row in Jason format, it's not particularly useful in CQ l s H running in the terminal. Nonetheless, this is the command that will return the data in this format. Let's exit secure L S H and shut down, Cassandra. One last thing regarding Jason let's take a second to browse through the course is CQ l file in the M five directory. You'll notice that some of the insert statements here used the Jason format. This is an example of using this key word and format when adding data to Cassandra

Conclusion
[Autogenerated] before we wrap up this module, let's step back and look at how we modeled. Our course related data, of course, has multiple modules, and a module is made above multiple clips. We've captured the relationship between a course and it's modules by using a composite primary key with a clustering key. We then modeled the relationship of a module with its clips, using a list with a user defined data type. In doing so, we've managed to define a table that captures three levels of information about a course storing all the data about a course in a single partition. That's pretty cool. In conclusion. In this module we introduced collections, including sets, lists and maps. We also looked at how to use T. T ells with these collections. Next, we examined two poles. We also introduced the frozen key word as a way to support nesting of complex data types. We rounded out the module with a look at user defined types and Cassandra's support for Jason. I hope this module has been a helpful introduction to the complex data types in Cassandra. Thank you

Making the Most of Cassandra
Materialized Views and Secondary Indexes
[Autogenerated] Hello. My name is Paul O'Fallon, and I'd like to welcome you to the next module in the course. Cassandra. For developers, this module is entitled making the most of Cassandra in this module. We're going to cover several distinct topics, each of which build on what we've covered so far and, in their own way, help round out the features offered by Cassandra. First, we'll compare and contrast materialized views and secondary indexes and how they help you query data in a variety of ways. Next, we'll examine Cassandra's support for grouping statements together in batches. After that, we'll take a look at how compare and set operations are supported. Using transactions finally will wrap up this module with a look at user defined functions and aggregates. Let's get started. So far, whenever we've queried data from a table, we've either crafted where clauses that follow the primary key structure or omitted the where Klaus altogether. However, Cassandra does support additional query methods, including support for materialized views. Let's look at an example with our users table. If we tried to query this table for user's working for a specific company, it would fail. However, we could create a materialized view called In this case, users by company, which would support this type of query, noticed a few things about this statement. First, this select star. When creating a view, you can also limit the number of columns from the source table that will be available in the view here. We've chosen to include all the columns from the user's table. Also notice the where company is not. No company is our partition key here, so obviously we can on Lee include Rose from the user's table that have a company. In fact, if you forget to include this, where Klaus Cassandra will warn you finally is our primary key statement we want to select by company. So that is our partition key. Additionally, the primary key of a materialized view must contain all the primary key elements of its source table. So we've included I D here as well. Okay, now, with that in place, we can query our materialized a view like this, specifying the view in the from Klaus and just the company in the wear clothes Great at the time of this course, riding materialized views are still experimental, but Cassandra has another method for achieving these additional query methods. Secondary indexes. These can be created with the Create Index command. Here we're creating an index on the company column in the user's table. Since these are index is created on the source table itself, we can now query directly against that table instead of a view with just a company name in our wear clause. So with two ways to accomplish basically the same thing, what's the difference? Secondary indexes were the first way Cassandra solved this problem, but they require the co ordinator noted to query every applicability data node looking for a match. This is because secondary indexes are created on a per node basis when querying users by company, the coordinator doesn't have a partition key to use when determining which node to contact. So it asks all potential candidates. On the other hand, clearing a user's by company view works just like a querying a table. In our example, the company is the partition key, and the coordinator note can use this to target the appropriate data node. There is one functional difference, however, at least at the time of this course, you can't create materialized views to help with searching within collections. However, you can create a secondary index for this purpose. Let's again extend our users table and allow users toe have tags Right now on Lee, courses can have tags like these here on the left. But let's say we want to allow users to identify themselves with tags as well. We'll do that by adding a Tags column to the user's table. With this in place, we can now create an index on that column just like we did with company earlier. Now, with this collection in place, as we answer rose into the user's table with tags, we can query users who have certain tags associated with their profile. Clearing indexed collections looks a little different, will select all the columns from users. But for the where Klaus will use the new key word contains to specify that we only want users who have a set of tags that contains the key word. Java. Clearing across all types of collections takes on two different forms. The first is where we create an index on a collection column. With these indexes, we can use the wear contains clause in our select statement. This works for all three collection types, sets, lists and maps for maps. Contains matches the values of the map. If you want to match on the keys of a map, you would create an index and use the keys Key word to specifically index the keys of the map. Then, when clearing the data you would use contains key instead of just contains this variant. Of course, on Lee makes sense for the map collection type.

Demo: Materialized Views and Secondary Indexes
[Autogenerated] in this demo will first add a materialized view, allowing us to query users by their company. Next, we'll do the same, but with a secondary index finally will demonstrate the support for secondary indexes of collections by adding a tags set to users. Let's get started. We'll start this demo by launching our single Cassandra Node with Docker composed up Dash D. Once that's up and running will pipe some CQ l commands from an earlier module to create a key space and users table. With that complete, we can run CQ L s H and used the plural site Key space. The first thing we'll do is alter our users table to add a company column. Then we'll update John Doe to indicate that he works for Acme Corporation. Now, if we try to select just the user's that work for acne, well, we get an error. Let's fix that First, by creating a materialized view, Our users by company view, has a partition key of company and a clustering key of I D. Remember that we must include the primary key of our underlying table in the primary key of this view as well. Our view is created. But we also see the warning that these are not recommended for production use yet. Now, if we select star from our newly created view, we do, in fact get back John Doe reflecting the update we just made. Now let's solve this another way with a secondary index will run a create Index Command to create an index on the company column. Now, with this index in place, we can query against the user's table directly specifying company in the wear clothes. Now let's add another column to the user's table. The tags column toe hold A set of Hvar car tags associated with the user With this column added, will run an update statement to add three tags to the user's table for John Doe. Let's say we want to find all the users who are tagged with Java that would require a wear. Claws of tags contains Java, however, that where Klaus doesn't work. Same error as before. Okay, so let's create another index for the user's table. This time on The Tags column running the same select again, and it works. We'll wrap up this demo by exiting Z Q l s H and shutting down our single Cassandra Node

Manually Maintained Indexes and Batches
[Autogenerated] Before we move on, we have another extra credit opportunity. Don't just take my word on whether Cassandra handles materialized reviews and secondary indexes differently. Check it out for yourself. You can do this by turning on, tracing and trying each option. First, launch a three node single data center cluster similar to those we used at the beginning of this course. Then create a plural psyche space with simple strategy and a replication factor of one. Create a user's table, a user by company, materialized view and a secondary index on the company column of the user's table. Add a few sample users with companies in turn on tracing. Try both selecting from the view as well as three users. Table with company in the where. Klaus, When looking at the tracing output, pay particular attention to which nodes are involved in each case, not it. Another option worth considering, especially until materialized views are ready for production is manually maintaining indexes yourself. By doing this, you can get performance closer to materialized views, but at the cost of maintaining multiple copies of the data yourself. To demonstrate this, let's add a set of tags to our courses. Table because of the structure of our table. We need to create our Tags column as a static column. Since it applies to the entire course, not just a single module. In order to query, all the courses that contain a certain tag will create our own index and manage it ourselves. It will just be a regular table with two columns. The tag, which will be the partition key and the course I D, which will be the clustering key. The two of these together will, of course, make up the primary key for the table. Now this does require more work on our apart when we insert or update our courses table. Whenever we insert a new course, it will contain a series of tags in the Tags column. After doing this, insert will need to do a series of subsequent inserts into our new index table, one for each tag that appeared in our courses. Insert statement. Now if we want a list, of course, is that have been assigned a certain tag. We can query our course tags table to get that back. So how do we ensure that our courses and course tags tables stay in sync well. That leads us to our next topic. Batches. People tend to think of batches in different ways, So let's start by covering the intended use case for Cassandra's batches. They're really intended to help keep related tables or related rose in the same table in sync. They're not necessarily intended to provide a high speed way of loading bulk data into Cassandra. A batch is basically a group of CQ L statements submitted together. As this batch is being processed by the coordinator node, it's progress is being communicated to other nodes in the cluster. This is to protect the integrity of the batch, and so another note can take over in the event the coordinator know dies before processing. All the statements contained in the batch on important distinction here is that a batch is not a transaction. There is no rollback feature, just an assurance that all the statements contained in the batch will be processed. Creating a batch is very easy. Just start with a begin batch statement and then issue seek ul commands as you would normally. Then, when you're done adding statements to the batch, finish with a KN apply batch command notice in our example here, we're using a batch to ensure that as we add tags to a course in our courses table, the corresponding inserts into our course tags table are being made as well. Cassandra also has the concept of an unlawful batch. This is a batch statement that doesn't do the multi node logging. As we showed earlier. This is most appropriate. When you're writing multiple rows into a single partition, the batch will be sent off to the appropriate data note as a single right, because there's just one right. There's really no distributed batch logging that needs to take place anyway. In our scenario, if we were to batch an insert to create a course along with a multiple inserts to create the individual modules, these could safely be combined into an unlocked batch. All of these inserts are headed for the same partition in our cluster. The partition identified by our course I D. Partition key. In the beginning, we mentioned that batches aren't necessarily the best way to do high speed loading of data into Cassandra on Alternative. To consider is to just use your client code to prepare an insert statement and then loop through your data, executing the prepared statement for each row. Many of the Cassandra client libraries have optimization sze that this approach can leverage to load data quickly, possibly even faster than if you were to batch it up together, since there's no multi node logging involved. Also, your client code has the opportunity to connect to many different coordinator nodes in order to paralyze the insert operations as much as possible.

Demo: Manually Maintained Indexes and Batches
[Autogenerated] in this demo will use Cassandra's support for batches to group inserts to both our courses and a new course tags table. Then we'll look at how we can use unlocked batches when loading our courses. Data. Let's get started. This should look familiar by now. We'll start by launching our single Cassandra node. After that, we'll load our complete courses data from the M five directory, then, once that's complete, will run CQ l s H and use plural site. Next, we'll add a similar Tags column to our courses table. Since tags relate to a course as a whole, we need to add the static keyword. We'll create a course tags table with a tag column and a course I D. Column. Now let's add a couple of tags to the Node.js big picture. Of course, we'll do all of these statements in a single batch, starting with begin batch, then updating our courses. Table inserting are two tags into our new course tags table and then wrapping up with apply batch. Running these commands doesn't really return anything interesting, but that's just fine. If we select rose from the course tags table for the tag value developer we see one of the rose we just inserted. Likewise. If we select the tags from our courses table for the know Js big picture course we see the two we just added with our updates statement. Now let's exit CQ l s h and shut down our one Cassandra note before we leave this demo, let's look at the courses dot c Q l file in the M six directory. This one is identical to the one we just loaded. Except the statements for each course are grouped into unlocked batches. Here, you can see begin unlocked batch at the start of the statements for the React big picture course. Then, after the last command related to that course on apply batch statement and then another begin unlocked a batch to mark the start of the next course.

Lightweight Transactions
[Autogenerated] more recent versions of Cassandra have included what they call lightweight transactions. This capability is an implementation of the Paksas algorithm. Any reasonable discussion of Pax O's is outside the scope of this course, but it is worth understanding that statements leveraging these transactions do come with a cost. Each transaction involves a leader node and several replica nodes. These notes communicate with each other in order to execute the transaction. In fact, it requires four round trip conversations between these leader and replica notes. The Prepare Promise step. The Reed results step the proposed except step and finally, the commit AC step. The first and third steps are core to the Paksas algorithm, while the second and fourth are extensions for Cassandra's specific use case, Cassandra exposes these transaction capabilities as compare and set operations. If you need to perform an insert and ensure that you're not inadvertently overriding an existing row, you can use the if not exists CQ all claws here. We want to create a new user as long as a user with that I D doesn't already exist in the table. Another use case involves updating a row. You can specify that an update be performed on a row on Lee. If a condition is true here, we only want to update the password and set the token to know if the token has the existing value we expect in this case. The column being compared is also part of the update statement, but that's not a requirement. You can evaluate an update. Different columns. Thes lightweight transactions work with matches, too. If the first statement in a batch is a compare and set operation and it is not applied such as because it already exists. As in this case, none of the remaining entries in the batch will be executed. There is one caveat, however. If you want to use lightweight transactions with batches, all the statements in the batch need to operate on the same table.

Demo: Lightweight Transactions
[Autogenerated] in this demo will perform a conditional insert into our users table Onley inserting a new user if they don't already exist. We'll also perform a conditional update as part of a password reset process. Let's get started. We'll store by running our Cassandra Node with Docker. Compose up Dash D. Once it's up and running, we can load our users table from the M three directory. Once that completes, we can run CQ l s H and use plural site. These are pretty straightforward demos. First, we'll try to insert a new user into the user's table, but with an existing i d of john hyphen dough. We've also added the if not exists to the end. As we discussed earlier inserts are up. Certs. That means without this qualifier on the end, it would just update the existing record. Executing this statement, though we see from our results that it was not applied, the first name is still John, not Johnny. Next will update the record for John Doe to set a password reset token. This token might be part of a your l e mailed to the user when they request a password reset. We wouldn't want to honor an attempt to re set a password that did not include this token. Now let's try to update John Doe's password to new password and remove the reset token. But on Lee, if the reset token equals an incorrect value notice in the results that the update was not applied and the reset token is still the original value, let's run that same statement. But with the correct reset token value. Now we see that the update was applied. Selecting John does password. We see the updated value will end this demo by exiting CQ l s H and shutting down our Cassandra node.

User Defined Functions and Aggregates
[Autogenerated] In addition to Cassandra's built in functions, it also supports user defined functions. These functions can be leveraged in select, insert an update statements. While Java and JavaScript are the only two languages supported out of the box, any Js are 2 23 Compliant language can be leveraged by adding the appropriate jar file to your Cassandra deployment. Thes user to find functions are part of your database schema, just like tables, indexes and views, and they are distributed around the cluster in the same fashion. Also a word of warning. Your code is executed inside the Cassandra _____ Process, which means a non performance function could impact your cluster. Therefore, user defined functions should be used with care. Here's a contrived example of a user defined function I say contrived because you probably wouldn't rely on a database function to do this. But it at least serves as an example of how to create and leverage a user to find function. In this use case, we want to form at the duration of our course in a human readable format. We'll do that by creating a function called as Time string. This will convert, for example, a duration of 24,936 seconds into the six hours and 55 minutes you see here. Not only can you create user defined the functions but also user to find aggregates. You'll remember that in an earlier module we looked at the men Max Count and average aggregates. User defined aggregates allow you to extend these with your own. They're structured as follows. They start with an initial condition, which is used to set the initial state of the aggregate. Then a state function is called for each row. Being aggregated this state function has access to the existing state and can update this state as well. Eventually, a final function may be called which, when given the final state, can return the results. Let's say we want to revisit our course page views table and generate a summary of course page views by our We can do this with a user defined aggregate. With an aggregate function called Hourly, we can return a map where the keys are the hour one through 24 and the values are the number of page views in that hour. The response shown here would indicate that there were four page views at 3 a.m. and two more at 10 a.m. Let's try creating these two examples now.

Demo: User Defined Functions and Aggregates
[Autogenerated] in this demo, we're going to create our as time string user to find function, to format our course durations in a human readable format, then will create an hourly user to find agri it to give us a summary of how many times a course is viewed each hour of the day. Let's get started before we launch our Cassandra note. Let's look at the doctor file we've been using in this module. It has an extra line which runs the UNIX, said command user defined functions are not enabled in Cassandra by default. And this command updates that Cassandra da Thiemo file in our doctor image to set both the enable user to find functions and the enable scripted user to find functions values too true. Okay, so now let's fire up a Cassandra instance with Docker composed up Dash D. Now we'll load our courses information, but this time from our M six directory using are unlocked batch statements. When that's done, will run CQ l s H and use plural site to create our user defined function as time string. I'm going to paste in the statement and then we'll go through it first. We have create or replace function, meaning that we can run this statement repeatedly to replace an existing function as well as use it to create the function in the first place. The parameters after the function name are those which are provided when the function is invoked in a c. Q. L statement. The first parameter here is the Inter jer duration itself. The second parameter is a flag indicating whether we want to include seconds in our output. The next line indicates that if our function is called with a no value issue just returned, no. After that, we indicate the data type are user defined function returns, a text or string value. After that, we specify the language of our function and proceed with the source code itself. The double dollar sign here is a Cassandra alternative to Single. Quoting a constant, I won't go through the source code line by line, but suffice it to say that it parses the duration, Enter Jer and returns a string value in a human readable format. I will say that writing this function directly in CQ Ellis age can be a difficult trial and error process. You'd do yourself a favor by writing and testing the function in your favorite i d before creating it as a function in Cassandra. Okay, now that we have our function in place, let's use it will select the name and human readable duration. For the first module of the advanced JavaScript course, we get back a nicely formatted string. Great. These functions can be nested. So let's calculate the sum of all the module durations for advanced JavaScript and then passed that value to our newly created function. Here we see ah, much longer duration of six hours and 55 minutes. And since we passed faults to our function, no seconds. So now that we've tackled a user to find function, let's try to create an aggregate. Will start by re creating our course page views table from an earlier module. Then we'll add some sample data to it as well. This will give us something to aggregate. An aggregate is actually composed of one or two separate user defined functions. The first function is called for each row. Here, we're creating a user to find function called hourly state, which is called with two parameters. The first is the current state of the aggregate and the second is the value we want to aggregate from the current row. Everything else should look roughly similar to our last user to find function. Example in the code were converting a time. You you i d into a date so we can extract the hour. The code here to convert the time You You i d is based on a data stacks support article which in turn references the Hector Cassandra client Note that the return value of this function is the state. Each time this function is called, it receives the state, plus the current rose value and returns and updated state. Okay, now it's time to create our aggregate. We could have also required a second function in our aggregate a final function to take the final state and return the results. However, in our case, it wasn't required. The final state itself is our results. Here we're creating or replacing an aggregate named hourly notice that we're on lee specifying one parameter here a time. You you i d which appears above as the second parameter to our hourly state function. Next, we specify the name of our state function hourly state and the format of our state, which is a map of entered your keys to represent the hour of the day and values of big int to represent the count of views for that hour. Finally, we set our initial condition to be an empty map. Now, with that in place, let's try using this new aggregate if we select the hourly views for our advanced JavaScript course. Well, uh, we get the same thing we saw in our slides earlier. That's pretty neat. We'll wrap up this demo by exiting CQ l s H and shutting down our Cassandra node.

Conclusion
[Autogenerated] so in conclusion, in this module we discussed materialized views and secondary indexes, highlighting the differences between the two. We then covered indexes of collections, something that's not supported in materialized views today. Next, we looked at how to use manually maintained indexes and how batches help when updating multiple tables at once. We then briefly touched on Cassandra's support for lightweight transactions. We wrapped up with an overview of user defined functions and aggregates. I hope this module and this course have given you a good idea of what's possible with Cassandra. Thank you.